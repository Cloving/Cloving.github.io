[{"title":"抓取网易云音乐榜单","url":"/2018/12/20/抓取网易云音乐榜单/","content":"\n\n## 摘要\n本文分别利用`Selenium`自动化测试工具与`requests`库抓取网易云音乐的榜单，包括歌曲名、歌曲时长以及演唱者等信息。`Selenium`仅仅抓取了首个榜单，`requests`抓取了所有榜单。抓取之后将所有数据存入`MySQL`数据库中。\n\n## Selenium抓取单个表单\n`Selenium`是一种自动化测试工具，它可以模拟浏览器的行为，所以也可以用于爬虫。下载对应的[驱动](https://sites.google.com/a/chromium.org/chromedriver/downloads)之后，可以调用各种主流浏览器。一般在爬虫的过程因为效率问题并不打开浏览器，而使用无界面的模式。\n\n### 执行过程\n#### 1、初始化变量\n初始化的过程中包括请求的`url`，请求的数目以及数据库的一些配置。`selenium`使用`Chrome`驱动。\n```Python\ndef __init__(self):\n  self.url = \"https://music.163.com\"\n  self.count = 0\n  self.host = \"127.0.0.1\"\n  self.user = \"root\"\n  self.password = \"******\"\n  self.db = \"spider\"\n  self.chrome_option = Options()\n  self.chrome_option.add_argument('--headless')\n  self.DRIVER = webdriver.Chrome(chrome_options=self.chrome_option)\n  # The format string is not really a normal Python format string.\n  # You must always use %s for all fields.\n  self.importSQL = \"\"\"INSERT INTO NetEaseCloudMusic(ID, musicName, time, singer)\n            VALUES (%s, %s, %s, %s)\"\"\"\n```\n#### 2、连接MySQL数据库，并从首页跳转到榜单页\n根据初始化的数据库配置信息，连接上数据库删除旧表，创建新表\n```python\ndef connect_mysql(self):\n  self.db = pymysql.connect(host=self.host, user=self.user, passwd=self.password, db=self.db)\n  print(\"成功连接Mysql数据库\")\n  self.cursor = self.db.cursor()\n  self.cursor.execute(\"DROP TABLE IF EXISTS NetEaseCloudMusic\")\n  # 如果使用单引号，需要\n  sql = '''CREATE TABLE NetEaseCloudMusic(\n          ID INT NOT NULL,\n          musicName LONGTEXT,\n          time LONGTEXT,\n          singer LONGTEXT\n          )'''\n  self.cursor.execute(sql)\n```\n\n榜单信息的url为`https://music.163.com/#/discover/toplist`，所以需要从首页跳转。跳转之后观察到榜单信息并不是直接加载在网页中，而是在页面的`iframe`标签中，所以需要跳转到子`iframe`拿到数据\n```python\ndef jump_targetpage(self):\n  # 其中 driver.get 方法会打开请求的URL，\n  # WebDriver 会等待页面完全加载完成之后才会返回，\n  # 即程序会等待页面的所有内容加载完成，JS渲染完毕之后才继续往下执行。\n  self.DRIVER.get(self.url)\n  self.DRIVER.find_element_by_xpath('//ul[@class=\"nav\"]/li[2]').click()\n  try:\n    self.firstChild_iframe = self.DRIVER.find_element_by_id(\"g_iframe\")\n  except Exception as e:\n    print(\"Get iframe Failed: \", e)\n  sleep(2)\n  self.DRIVER.switch_to.frame(self.firstChild_iframe)\n  table = self.DRIVER.find_element_by_tag_name('table')\n  self.songList = table.find_elements_by_xpath('tbody/tr')\n  return self.songList\n```\n\n#### 3、提取数据并保存到数据库中\n使用`xpath`来提取歌曲名称、播放时长、演唱者等信息。\n```Python\ndef start_search(self, song):    \n  songChildList = []\n  songChildList.append(song.find_element_by_xpath('td[2]//b').get_attribute('title'))\n  songChildList.append(song.find_element_by_xpath('td[3]//span[@class=\"u-dur \"]').text)\n  songChildList.append(song.find_element_by_xpath('td[last()]//span').get_attribute('title'))\n  return songChildList\n```\n\n保存到数据库中，并记录每次导入的条数\n```python\ndef import_data(self, data):\n  try:\n    self.cursor.execute(self.importSQL, (self.count+1, data[0], data[1], data[2]))\n    self.db.commit()\n    self.count += 1\n    print(\"成功导入第{count}条数据\".format(count=self.count))\n  except Exception as e:\n    print('导入数据失败： ', e)\n    self.db.rollback()\n```\n### 小结\n这里并不介绍`Selenium`的具体使用方式，侧重于抓取的过程描述以及如何实现这一过程的简单代码示例。完整代码：[Selenium抓取单个表单](https://github.com/Cloving/NetEase-Spider/blob/master/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E6%A6%9C%E5%8D%95/selenium_NetEaseMusic.py)\n\n## requests抓取所有表单\n分析网易云音乐榜单页面的网络请求，如图所示：\n![默认页面请求](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取网易云音乐榜单/默认页面请求.jpg)\n\n从默认页面`https://music.163.com/#/discover/toplist`请求的响应信息可以看到各类榜单的标题以及`href`属性信息。事实上，通过观察浏览器地址栏的`url`可以看出`href`的属性值位于`#`之后，所以这是一个`fragment`，访问网站之后可以直接到达页面的指定位置。我们需要提取出每个榜单的`href`属性信息，再逐个请求这些`url`，通过返回的响应分析出每个`url`中包含的歌曲信息，再逐个保存到数据库中，并统计总的数据量。\n\n### 请求过程\n初始化变量以及连接数据库的部分在使用`selenium`请求中已经介绍过，大同小异。此处不再过多介绍，在文章结尾处会给出源代码的链接。\n关于发起请求的函数部分，由于请求的`url`包括初始的`url`以及每个榜单的`url`，所以在请求函数中考虑使用可变数量的参数，如果没有传入其他参数则使用初始化的`url`，否则使用传入的`url`。过程如下：\n\n``` python\ndef start_request(self, *arg):\n  # 向给定的url请求数据，如果arg为空，请求原始的url，否则请求arg\n  url = arg if arg else self.url\n  if isinstance(url, tuple):\n    # arg是元组类型\n    request_url = url[0]\n  else:\n    request_url = url\n  try:\n    response = requests.get(request_url, headers=self.header)\n    if response.status_code == 200:\n      print(\"请求成功\")\n    return response\n  except Exception as e:\n    print(\"请求失败，失败原因：\", e)\n```\n\n### 处理原始请求的数据\n对原始的`url`发送请求时，使用`正则表达式`提取所有榜单的`href`属性，再与原始的`url`进行拼接（因为拼接的`url`即使不带有`#`也会自动跳转，所以代码中并未添加`#`），得到并请求单个榜单的`url`。\n单个榜单的`url`中并不能够找到规则的歌曲数据。通过观察发现所有歌曲数据都存在其中一个`textarea`的标签中，标签中的数据为`json`格式，所以需要利用这些`json`数据提取歌曲信息。这里通过`正则表达式`将其提取出来。\n\n\n![单个榜单请求](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取网易云音乐榜单/单个榜单请求.png)\n\n``` python\ndef process_data(self, source_response):\n  # 处理原始url的请求，提取并处理每个榜单的url\n  pattern = re.compile('<p class=\"name\"><a href=\"(.*?)\" class=\"s-fc0\">')\n  lists_link = pattern.findall(source_response.text)\n  for list_link in lists_link:\n    url = parse.urljoin(self.url, list_link)\n    list_response = self.start_request(url)\n    json_datas = self.process_jsondata(list_response)\n    self.process_musicdata(json_datas)\n\ndef process_jsondata(self, response):\n  # 提取每个榜单的json数据 \n  pattern = re.compile('.*?id=\"song-list-pre-data\".*?>(.*?)</textarea>')\n  match = pattern.search(response.text)\n  json_datas = json.loads(match.group(1))\n  with open('data.json', 'wb') as f:\n    f.write(match.group(1).encode('utf8'))\n  return json_datas\n```\n\n### 处理包含歌曲信息的json数据\n歌曲名与演唱者都可以直接从`json`中直接方便的读取到，但是歌曲的时长并不能从数据中直接看出来。所以回到原始榜单的响应中寻找信息，发现了可能是控制歌曲时长显示的部分，如图所示：\n\n![歌曲时长](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取网易云音乐榜单/歌曲时长.png)\n\n以下语句为控制歌曲时长显示的代码：\n``` javascript\n${dur2time(x.duration/1000)}\n```\n\n实际上我并没有在所有网络请求中找到有关`dur2time`的信息（也可能是我还不知道怎样分析这一部分）。但在`json`数据中存在`duration`的`key`，所以我认为`duration`的数值可能会通过某种转换，显示为歌曲时长。将`duration`的值除以1000与页面中显示的歌曲时长对应起来，试着找到其中的规律。\n\n- 179.2 &nbsp;&nbsp;  02：59\n- 216 &nbsp;&nbsp;  03：36\n\n从以上的对应关系中基本上可以判断`duration`除以1000即是歌曲时长的秒数。\n到此可以便可以对歌曲的数据进行提取。过程如下所示：\n\n```python\ndef process_musicdata(self, json_datas):\n  # 处理每个榜单的json数据\n  for json_data in json_datas:\n    music_name = json_data.get(\"name\")\n    play_time = self.process_playtime(json_data.get(\"duration\")/1000)\n    singers = ''\n    for artist in json_data.get(\"artists\"):\n      singers += '/' + artist.get(\"name\")\n    singers = singers.replace('/', '', 1)\n    data = [music_name, play_time, singers]\n    self.save_data(data)\n\ndef process_playtime(self, duration):\n  # 处理播放时间数据\n  minutes = math.floor(duration/60)\n  seconds = math.floor(duration%60)\n  minutes = minutes if minutes >= 10 else '0'+str(minutes)\n  seconds = seconds if seconds >= 10 else '0'+str(seconds)\n  return '%s:%s' % (minutes, seconds)\n```\n\n### 保存到数据库中\n每次处理一个榜单的响应便保存到数据库中。本次提取的所有歌曲榜单，共有`1614`条音乐数据\n```python\ndef save_data(self, data):\n  # 保存数据到数据库\n  try:\n    self.cursor.execute(self.importSQL, (self.count, data[0],data[1],data[2]))\n    self.db.commit()\n    print(\"成功导入第{count}条数据\".format(count=self.count))\n    self.count += 1\n  except Exception as e:\n    print(\"导入数据失败，失败原因：\", e)\n    self.db.rollback()\n```\n\n## 总结\n本文分别使用了`Selenium`与`requests`两种方式抓取音乐榜单数据。实际测试中`Selenium`的抓取速度较慢，但是过程相对简单。使用`requests`请求过程稍显复杂，但是速度较快。完整代码：[抓取网易云音乐榜单](https://github.com/Cloving/NetEase-Spider/tree/master/%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E6%A6%9C%E5%8D%95)\n\n\n## 参考文献\n[1、Selenium with Python Selenium Python Bindings 2 documentation](https://selenium-python.readthedocs.io/)\n[2、Getting Started with Headless Chrome | Web | Google Developers](https://developers.google.com/web/updates/2017/04/headless-chrome)\n[3、2. 快速入门 Selenium-Python中文文档 2 documentation](https://selenium-python-zh.readthedocs.io/en/latest/getting-started.html)\n[4、python3 urllib.parse.urljoin（）用法 - 余安 - CSDN博客](https://blog.csdn.net/mycms5/article/details/76902041)\n[5、20.7. urllib.parse — Parse URLs into components &mdash; Python v3.2.6 documentation](https://docs.python.org/3.2/library/urllib.parse.html)\n\n\n","tags":["Python","正则表达式","MySQL","Selenium"],"categories":["数据抓取","网易云音乐"]},{"title":"抓取知乎用户粉丝数据","url":"/2018/12/13/抓取知乎用户粉丝数据/","content":"\n## 摘要\n本文抓取并分析了知乎用户的粉丝数据，包括粉丝名、粉丝标题、粉丝男女比例、粉丝回答问题比例、粉丝被关注数等数据。之后利用`pyecharts`库对这些数据进行可视化显示。\n<br/>\n## 抓取数据\n### 1、请求的url与代理\n与之前一篇文章[抓取知乎用户动态数据](http://yaodongsheng.com/2018/11/20/抓取知乎用户动态数据/)类似，这里请求的url也是返回json数据，所以同样在控制台network选项下找到请求的url，通过观察多个请求的url将其分为通用部分与扩展部分，如下形式：\n```Python\nself.base_url = \"https://www.zhihu.com/api/v4/members/{user}/followers?{include}\"\nself.include = 'include=data%5B*%5D.answer_count%2Carticles_count%2C\\\n  gender%2Cfollower_count%2Cis_followed%2Cis_following%2C\\\n  badge%5B%3F(type%3Dbest_answerer)%5D.topics&offset={offset}&limit=20'\n```\n其中`user`为用户的个性化域名，`include`为扩展部分的url，`offset`为偏移量，即从第`offset`条数据开始。\n代理使用[fake-useragent](https://pypi.org/project/fake-useragent/)库，每次请求都随机生成，保证每次请求的`header`都不一样。\n<br/>\n### 2、设置循环抓取\n首先根据url请求`json`数据，之后处理请求到的数据，得到一部分粉丝的信息，如果粉丝信息没有抓取完，则对抓取的`url`进行更新，直到数据抓取完成，结束的标志会在请求的`json`数据中获取到。最后对提取到的所有数据进行保存。代码如下：\n```Python\ndef start(self, username, savepath, savename):\n  index = 0\n  flag = True\n  while flag:\n    offset=index*20\n    include = self.include.format(offset=offset)\n    url = self.base_url.format(user=username, include=include)\n    # 请求json数据\n    print(\"正在进行第{}次请求\".format(index+1))\n    jsonData = self.requests_source(url)\n    # 提取需要的数据\n    self.process_data(jsonData)\n    # 如果仍然存在数据，处理下一次请求\n    if jsonData.get('paging').get('is_end') is False:\n      index = index + 1\n      self.header['user-agent'] = self.agent.random\n    else:\n      flag = False\n      print(\"完成请求\")\n  # 保存数据\n  self.save_data(self.data, savepath, savename)\n```\n<br/>\n### 3、请求、处理数据\n请求处理的过程并不复杂，根据返回的状态码判断是否成功拿到`json`，之后会根据`json`的具体形式解析数据。\n```Python\ndef requests_source(self, url):\n  try:\n    response = requests.get(url, headers=self.header, timeout=(5,30))\n    if response.status_code == 200:\n      return response.json()\n  except requests.ConnectionError as e:\n    print(\"获取请求失败，失败原因：\", e)\n\ndef process_data(self, jsonData):\n  followers = jsonData.get('data')\n  for follow in followers:\n    name = self.read_data_from_jsonData(follow, 'name')\n    headline = self.read_data_from_jsonData(follow, 'headline')\n    gender = self.read_data_from_jsonData(follow, 'gender')\n    follower_count = self.read_data_from_jsonData(follow, 'follower_count')\n    answer_count = self.read_data_from_jsonData(follow, 'answer_count')\n    self.data[name] = [headline, gender, follower_count, answer_count]\n    print(name, self.data[name])\n  print('\\n')\n\ndef read_data_from_jsonData(self, json, key):\n  value = json.get(key)\n  return value if value is not '' else 'Empty'\n```\n<br/>\n### 4、保存数据\n这里将所有信息保存到`pkl`文件当中\n```Python\ndef save_data(self, follower_data, savepath, savename):\n  if not os.path.exists(savepath):\n    os.mkdir(savepath)\n  with open(os.path.join(savepath, savename), 'wb') as f:\n    pickle.dump(follower_data, f)\n  f.close()\n  print(\"完成存储\")\n```\n完成所有信息的抓取、处理及保存之后下一步就是解析可视化这些数据。\n<br/>\n## 可视化数据\n可视化数据利用了[pyecharts](http://pyecharts.org/)库与[jieba](https://pypi.org/project/jieba/)库，前者用于可视化各类图表，后者是Python的中文分词组件。\n<br/>\n### 1、打开`pkl`文件\n保存`pkl`文件的名称可以自行指定，每次抓取如果文件名称改变那么分析数据的时候需要修改代码，所以使用了正则匹配，这样无需关心pkl文件的名称就可以直接运行。代码如下：\n```Python\nfor root, dirs, files in os.walk(os.path.join(os.path.dirname(__file__),'result')):\n  # print(root)  # 当前目录路径\n  # print(dirs)  # 当前路径下所有子目录\n  # print(files)  # 当前路径下所有非目录子文件\n  for file in files:\n    try:\n      file_name = re.search('(.*?).pkl', file).group()\n      break\n    except Exception as e:\n      continue\nwith open('./result/'+file_name, 'rb') as f:\n  followers_data = pickle.load(f)\n```\n<br/>\n### 2、提取数据\n为了便于分析、可视化数据，所以会将所有数据提取成`[(元组1), (元组2),...,(元组3)]`的形式，例如性别数据的形式为\n```Python\n[('男性': 32),('女性': 42),('保密': 52)]\n```\n\n生成`headline`与`用户名`的词云图时使用`jieba`库分词，其中在处理`headline`时需要删除掉一些符号字符之后再统计词频\n```Python\ndef statistic_word_frequency(self, texts, stopwords):\n  # 统计词频\n  statistic_dict = {}\n  for text in texts:\n    temp = jieba.cut(text, cut_all=False)\n    for t in temp:\n      if t in stopwords or t == 'Empty':\n        continue;\n      if t in statistic_dict.keys():\n        statistic_dict[t] += 1\n      else:\n        statistic_dict[t] = 1\n  return list(statistic_dict.items())\n```\nstopwords用来存储需要删除掉的一些字符。\n<br/>\n\n### 3、示例图\n示例图是随机寻找的一位知乎用户进行分析的结果\n![标题词云图](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取知乎用户粉丝数据/标题词云图.png)\n![用户名词云图](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取知乎用户粉丝数据/用户名词云图.png)\n![粉丝被关注数量直方图](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取知乎用户粉丝数据/粉丝被关注数量直方图.png)\n![粉丝回答数量直方图](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取知乎用户粉丝数据/粉丝回答数量直方图.png)\n![粉丝男女比例饼图](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取知乎用户粉丝数据/粉丝男女比例饼图.png)\n\n<br/>\n## 总结\n本文的思路参考了知乎用户[Charles](https://www.zhihu.com/people/charles_pikachu/activities)的爬虫项目。最终完成的代码：[知乎用户粉丝](https://github.com/Cloving/zhihu-Spider/tree/master/%E7%9F%A5%E4%B9%8E%E7%94%A8%E6%88%B7%E7%B2%89%E4%B8%9D)\n\n\n## 参考文献\n1、[Charles的皮卡丘](https://mp.weixin.qq.com/s/hWUvMFIAMBrgR2Is9P5QXw)\n2、[pyecharts - A Python Echarts Plotting Library](http://pyecharts.org/#/zh-cn/charts_base)\n3、[fxsjy/jieba: 结巴中文分词](https://github.com/fxsjy/jieba)\n","tags":["Echarts","Python","正则表达式","词云"],"categories":["数据抓取","知乎"]},{"title":"Echarts足迹图","url":"/2018/12/08/Echarts足迹图/","content":"\n## 摘要\n`Echarts`是百度EFE可视化团队开发的基于`Javascript`开源可视化图表库。由于部分blog文章中有展示图表的需求，基于此，本文研究了如何将`Echarts`的图表嵌入到blog中去。\n<br/>\n## Hexo插件\n[hexo-tag-echarts3](https://github.com/kchen0x/hexo-tag-echarts3)是一款在Hexo博客中导入echarts图表的插件。但是它不支持导入地图，同时该插件要求所有渲染的代码都必须在`option`的内部（变量`option`一般用于指定了图表的配置项和数据）。如果有调用外部函数、外部变量的情况，效果可能就不太好。所以对插件进行了修改，支持了地图功能，同时改进了渲染机制，即渲染代码只含有`option`，或者除`option`外还调用了外部的函数的情况都可以成功执行。\n\n<br/>\n## 插件修改\n### 引入地图\n首先需要修改引入的`echarts`文件，因为`echarts`通用包中不含有地图组件，所以必须使用完整包或者[在线定制](http://echarts.baidu.com/builder.html)的方式\n```JavaScript\n<script type=\"text/javascript\" src=\"https://cdn.bootcss.com/echarts/4.2.0-rc.2/echarts.min.js\"></script>\n```\n\n另Echarts现在不能够直接下载地图，所以需要能够找到外部地图js文件。可以从github上下载[China.js](https://github.com/apache/incubator-echarts/blob/master/map/js/china.js)再从本地导入。\n例如：下载到`/themes/主题名/source/js/`中，导入方式为：\n```JavaScript\n<script type=\"text/javascript\" src=\"/js/echarts.js\"></script>\n```\n或者引用线上文件：\n```JavaScript\n<script type=\"text/javascript\" src=\"http://gallery.echartsjs.com/dep/echarts/map/js/china.js\"></script>\n```\n**注意：**根据`<script>`加载机制，引用顺序必须先引入`echarts.js`，后载入地图js文件\n\n<br/>\n### 修改渲染方式\n修改模板文件`template.html`代码\n```javascript\n<script type=\"text/javascript\">\n  // 基于准备好的dom，初始化echarts实例\n  var myChart = echarts.init(document.getElementById('<%- id %>'));\n  // 指定图表的配置项和数据\n  <%= sourceCode %>\n  // 使用刚指定的配置项和数据显示图表。\n  myChart.setOption(option);\n</script>\n```\n\n修改`index.js`文件\n```JavaScript\nfunction echartsMaps(args, content) {\n    var template = fs.readFileSync(filePath).toString(),\n        options = {};\n    if (content.length) {\n        var options = content;\n    }\n    return _.template(template)({\n        id: 'echarts' + ((Math.random() * 9999) | 0),\n        sourceCode: content,\n        height: args[0] || 400,\n        width: args[1] || '85%'\n    });\n}\n```\n其中参数`content`为嵌入的代码，以上修改会将嵌入的代码直接渲染，之后调用`myChart.setOption(option);`使用配置项和数据显示图表。\n<br/>\n## 渲染图\n\n![足迹](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/hexo-tag-echarts-chart插件/足迹.png)\n\n<br/>\n## 总结\n本文叙述了如何将`Echarts`的图表嵌入到blog中，由于我需要使用足迹地图并且会调用`option`外部的函数，所以修改了原来的插件。修改后的插件地址：[hexo-tag-echarts-chart](https://github.com/Cloving/hexo-tag-echarts-chart)，欢迎试用。\n\n<br/>\n## 参考文献\n1、[使用 ECharts3.0 在 Hexo 搭建的博客中建一个足迹👣页面 | Docle の Blog](https://docle.github.io/2018/04/06/Use-ECharts-To-Build-A-[footprint]-Page/)\n2、[在 Hexo 中插入 ECharts 动态图表 - KChen's Blog](https://kchen.cc/2016/11/05/echarts-in-hexo/)\n3、[谈谈script标签以及其加载顺序问题，包含 defer async  - 个人文章 - SegmentFault 思否](https://segmentfault.com/a/1190000013615988)\n4、[浏览器加载 JS 文件的先后顺序同具体的解析和执行有什么关系？ - 知乎](https://www.zhihu.com/question/20531965)\n5、[kchen0x/hexo-tag-echarts3: A simple plugin for inserting ECharts 3 by using tags in Hexo](https://github.com/kchen0x/hexo-tag-echarts3)\n<br/>\n\n","tags":["Echarts","Javascript"],"categories":["建站实录","足迹图"]},{"title":"Python模拟豆瓣登录（一）","url":"/2018/11/30/Python模拟豆瓣登录（一）/","content":"<br/>\n# 摘要\n抓取数据的过程中有时候需要完成模拟登录的操作，本文使用`requests`库完成豆瓣的模拟登录，并保存已登录的Cookie，方便下次直接登录。之后通过访问个人主页验证当前状态是否为已登录状态。\n\n![登录页面](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/豆瓣登录（一）/登录页面.png)\n\n<br/>\n# requests请求\n\n## 分析过程\n首先需要研究豆瓣的登录机制，在豆瓣的登录界面查看登录请求提交的表单项，如图所示：\n\n![登录表单](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/豆瓣登录（一）/登录表单.png)\n\n图中的`Login`请求为post类型，`Form Data`表示post到服务器的数据，可以看出数据并未加密。\n\n1、`source`表示该登录页面是由豆瓣读书跳转过来\n2、`redir`表示登录后跳转到的url\n3、`form_email`和`form_password`分别表示用户名和密码\n4、`captcha-solution`则是验证码\n5、`captcha-id`验证码的id该字段可从登录页面的HTML中获取\n\n## 编码过程\n### 1、初始化变量：\n```Python\ndef __init__(self):\n  self.session = requests.Session()  \n  self.session.cookies = cookielib.LWPCookieJar(filename=\"cookies.txt\")  \n  self.url = 'https://accounts.douban.com/login'  # 登录url\n  self.redirurl = 'https://book.douban.com/mine'  # 重定向url\n  self.email = '******'             \n  self.password = '******'\n  # 构造post数据\n  self.data = {\n    'redir': self.redirurl,\n    'form_email': self.email,\n    'form_password': self.password,\n    'login': '登录'\n  }\n  # 构造用户代理\n  self.headers = {\n    'User-Agent': 'Mozilla/5.0 '\n    '(Windows NT 10.0; Win64; x64) '\n    'AppleWebKit/537.36 (KHTML, like Gecko) '\n    'Chrome/55.0.2883.87 Safari/537.36'\n  }\n```\n\n### 2、发送请求\n我们模拟登录需要的是登录之后的页面，所以利用`Session`来维持一种会话状态，并且保存登录后的`Cookie`。下次登录时直接携带`Cookie`发送请求，无需再使用账号密码。判断登录时是否需要验证码先使用`BeautifulSoup`库来获取登录页面的`HTML`代码，之后利用正则表达式判断其中是否有显示验证码的`img`标签，如果有处理验证码，如果没有那么就将已有的信息post到服务器，并保存登录后的Cookie。\n\n```Python\ndef login(self):\n  page = self.session.post(self.url, headers=self.headers)\n  soup = BeautifulSoup(page.text, \"html.parser\")\n  captcha = soup.find('img', id='captcha_image')\n  if captcha is not None:\n    self.process_captcha(page, captcha)\n    afterLogin_page = self.session.post(self.url, data=self.data, headers=self.headers)\n  else:\n    afterLogin_page = self.session.post(self.url, data=self.data, headers=self.headers)\n  # print(self.session.cookies)\n  self.session.cookies.save(ignore_discard=True, ignore_expires=True)\n  print(afterLogin_page.text)\n  soup = BeautifulSoup(afterLogin_page.text, \"html.parser\")\n```\n\n#### 关于Cookie与Session\nHttp是一种无状态的协议，因此并不能追踪用户的状态，所以常采用`Session`与`Cookie`结合的方式跟踪用户的状态。`Session`位于服务器端只保存对话信息但是不能够识别出具体的用户，`Cookie`位于客户端用于存放用户信息。`Cookie`在登录网站时会由服务器产生传递给客户端，当客户端再次登录时会携带`Cookie`，之后服务器便根据`Cookie`中的`Session ID`跟踪到会话。如果会话有效，那么会判断用户已处于登录状态，否则可能会判断用户没有访问权限进而跳转到登录页面。\n同时Cookie是有过期时间的，超过该时间Cookie失效，需要重新获取。这同时也是为了避免Cookie被他人获取并长期使用。\n\n### 3、处理验证码\n这里先使用手动输入验证码的方式，首先拿到验证码图片的地址（这个地址是临时的，一段时间后会失效，但对于手动输入并不影响），然后利用正则表达式拿到`captcha-id`，因为它需要与验证码一同post到服务器。具体代码如下：\n\n```Python\n  def process_captcha(self, page, captcha):\n    # 处理验证码\n    # 获得验证码图片地址\n    captcha_url = captcha['src']\n    # 利用正则表达式获得验证码ID\n    pattern = re.compile('<input type=\"hidden\" name=\"captcha-id\" value=\"(.*?)\"/')\n    captcha_id = re.search(pattern, page.text).group(1)\n    # 将验证码图片保存到本地\n    urllib.request.urlretrieve(captcha_url, \"captcha.png\")\n    try:\n      image = Image.open('captcha.png')\n      image.show()\n      image.close()\n    except Exception as e:\n      print(\"打开验证码图片失败，请手动重试\")\n    captcha = input('please input the captcha:')\n    self.data['captcha-solution'] = captcha\n    self.data['captcha-id'] = captcha_id\n```\n\n### 4、利用Cookie登录\n载入本地Cookie之后再发送get请求到url，之后将请求到的`HTML`文本写入文件。\n**注意：**以二进制的类型写入需要bytes对象，所以使用`utf8`编码\n```Python\ndef get_index(self):\n  # 根据本地cookies登录\n  try:\n    self.session.cookies.load(ignore_discard=True)\n  except Exception as e:\n    print(\"cookie未能加载, 原因: \", e)\n  response = self.session.get(self.redirurl, headers=self.headers)\n  with open(\"index.html\", 'wb') as f:\n    f.write(response.text.encode('utf8'))\n  print(\"已载入本地Cookie\")\n```\n\n# 总结\n过程比较简单，编码过程也不复杂，验证码的处理暂时使用手动的方式。关于自动识别豆瓣验证码的方式正在研究中，目前效果还不理想。\n完整代码：[douban_login_1.py](https://github.com/Cloving/Douban-Spider/blob/master/%E8%B1%86%E7%93%A3%E7%99%BB%E5%BD%95/douban_login_1.py)\n\n# 参考文献\n1、[python 爬虫 cookie 的保存和加载 - 盖娅 - 开源中国](https://my.oschina.net/sukai/blog/662046)\n2、[Python爬虫基础练习(十一)简单模拟豆瓣登录 - 知乎](https://zhuanlan.zhihu.com/p/38191385)\n<br/>\n","tags":["Python","验证码","模拟登录","正则表达式"],"categories":["数据抓取","豆瓣"]},{"title":"抓取知乎用户动态数据","url":"/2018/11/20/抓取知乎用户动态数据/","content":"\n<br />\n## 一、简介\n打开知乎某个用户的主页可以看到该用户赞同回答、关注专栏、关注问题等行为数据。本文利用Python的requests库抓取知乎用户行为数据并存入MongoDB数据库中。\n<br/>\n## 二、分析数据加载方式\n1. 打开网页后向下拉取可以观察到行为数据并不是一次加载完，而是随着用户的浏览进度逐步加载，考虑可能是ajax的形式。\n2. 之后打开Chrome控制台分析网络选项，查看XHR(XMLHttpRequest)类型的文件可以观察出收到的响应是json数据，其中包含了用户行为数据的相关信息\n\n![图1](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取知乎用户动态数据/知乎分析1.png)\n\n所以首先需要请求到网站返回的json数据，然后对数据进行解析，结构化处理，存入MongoDB中。\n<br/>\n## 三、代码分析\n### 1、初始化变量\n```python\n  def __init__(self):\n    self.user_name = \"******\"\n    self.max_search_counts = **\n    self.params = {\n      \"limit\": 7,\n      \"desktop\": \"True\"\n    }\n    self.base_url = \"https://www.zhihu.com/api/v4/members/\"\n    self.url = self.base_url + '/' + self.user_name + '/activities?' + urlencode(self.params)\n\n    self.headers = {\n      \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64)\\\n                    AppleWebKit/537.36 (KHTML, like Gecko)\\\n                    Chrome/70.0.3538.77 Safari/537.36\",\n      \"Referer\": \"https://www.zhihu.com/\",\n      \"x-requested-with\": \"fetch\"\n    }\n    self.client = MongoClient('127.0.0.1',username='admin',\n                        password='******',\n                        authSource='admin',\n                        authMechanism='SCRAM-SHA-1')\n    self.db = self.client[\"zhihu\"]\n    self.collection = self.db[\"zhihuDynamic\"]\n```\n\n1、`user_name`并不是用户的知乎ID，而是用户的个性化域名。比如知乎小管家的个性化域名为`zhihuadmin`，这可以通过观察用户主页的url获得\n2、请求的url就是返回json数据，在控制台的`Network`选项中观察到返回`json`数据的`url`类似于以下形式：\n```html\nhttps://www.zhihu.com/api/v4/members/zhihuadmin/activities?limit=7&after_id=1542365521&desktop=True\n```\n分析组成可以看出`https://www.zhihu.com/api/v4/members/`为通用部分。`zhihuadmin`为用户的个性化域名。?之后是携带的参数\n3、抓取知乎的数据必须携带用户代理。\n\n### 2、开始抓取\n```Python\ndef startSearch(self):\n  for search_times in range(0, self.max_search_counts):\n    json = self.get_Page(self.url)\n    try:\n      self.url = json.get(\"paging\").get(\"next\")\n      print(self.url)\n    except Exception as e:\n      print(\"获取下一次请求的链接失败，失败原因：\", e)\n    results = self.get_Parse(json)\n    print(\"=======================================\")\n    for result in results:\n      self.save_to_mongodb(result)\n```\n`for`循环控制搜索次数，每次搜索会抓取7条数据即7次动态。之后提取出本次抓取到json数据中的用户动态信息，并更新下一次搜索的url，并将提取出的数据存入MongoDB中\n\n### 3、抓取json数据\n```Python\n# 获取json数据\ndef get_Page(self, url):\n  try:\n    response = requests.get(url, headers=self.headers)\n    if response.status_code == 200:\n      return response.json()\n  except requests.ConnectionError as e:\n    print(\"Error: \", e.args)\n```\n传入页面的url,返回相应的json数据\n\n\n### 4、解析json数据\n\n根据传入json数据的格式，提取用户的各项动态\n![json数据示意图]( https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取知乎用户动态数据/知乎json.png)\n\n```Python\n# 解析json数据\ndef get_Parse(self, json):\n  if json:\n    items = json.get('data')\n    for item in items:\n      zhihu = {}\n      action_text = item.get(\"action_text\")\n      created_time = item.get(\"created_time\")\n      zhihu[\"操作行为\"] = action_text\n      zhihu[\"时间\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(created_time))\n      if item.get('verb') == \"MEMBER_VOTEUP_ARTICLE\":\n        item_detail =  voteup_article(item.get(\"target\"))\n      elif item.get('verb') == \"ANSWER_VOTE_UP\":\n        item_detail = answer_voteUp(item.get(\"target\"))\n      elif item.get('verb') == \"MEMBER_CREATE_ARTICLE\":\n        item_detail =  create_article(item.get(\"target\"))\n      elif item.get('verb') == \"MEMBER_FOLLOW_COLUMN\":\n        item_detail =  follow_column(item.get('target'))\n      elif item.get('verb') == \"ANSWER_CREATE\":\n        item_detail = answer_create(item.get('target'))\n      elif item.get('verb') == \"QUESTION_FOLLOW\":\n        item_detail = question_follow(item.get('target'))\n      yield dict(zhihu, **item_detail)\n```\n\n### 5、将解析出来的数据导入到数据库中\n```Python\n  # 导入到json数据库中\n  def save_to_mongodb(self, result):\n    if self.collection.insert_one(result):\n      print(\"Successful save to Mongodb\")\n```\n<br/>\n## 四、总结\n代码写的并不好，仅作为参考。完整代码: [点这里](https://github.com/Cloving/zhihu-Spider/tree/master/%E7%9F%A5%E4%B9%8E%E7%94%A8%E6%88%B7%E5%8A%A8%E6%80%81)\n<br/>\n## 参考文献\n### 1、[Authentication Examples — PyMongo 3.7.2 documentation](http://api.mongodb.com/python/current/examples/authentication.html)\n### 2、[Ajax结果提取 — Python3网络爬虫开发实战](https://germey.gitbooks.io/python3webspider/content/6.3-Ajax%E7%BB%93%E6%9E%9C%E6%8F%90%E5%8F%96.html)\n### 3、[知乎小管家 — 知乎](https://www.zhihu.com/people/zhihuadmin/activities)\n<br/>\n","tags":["Python","ajax","MongoDB"],"categories":["数据抓取","知乎"]},{"title":"三寸气在千般用，一旦无常万事休","url":"/2018/09/21/三寸气在千般用，一旦无常万事休/","content":"\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=574919767&auto=1&height=66\"></iframe>\n\n<br/>\n## 其实人生很长，\n### 选择一个自己真正想要从事的领域，沉淀下去，把自己变得更厉害一点。\n### 因为对于人来说不怕有缺点，就怕没特点。\n### 但不要想着一定要比谁谁更好，因为不论你多努力，总有人比你厉害，也总有人永远不如你。\n### 和别人比较大概是生而为人最可悲的事。\n### 总而言之，不应当有太多的欲望，不要做了一件事就必须要得到什么。但行好事，莫问前程。\n<br/>\n## 其实人生也很短，\n### 生命是脆弱的，一次看似普通的熬夜也许就榨干了你全部的精力，\n### 一次看似普通的疾病也许会换来了一张薄薄的病危通知书。\n### 珍视自己，珍视珍视你的人，不做让自己后悔的事。\n<br/>\n<br/>","tags":["念念不忘，必有回响"],"categories":["念念不忘，必有回响"]},{"title":"二分搜索算法","url":"/2018/06/13/二分搜索算法/","content":"\n\n## 1.1 基本的二分搜索(递归)\n\n\n```c++\n// 普通二分：递归\nint BSearch1(vector<int> ve, int target) {\n  int left = 0, right = ve.size()-1;\n  if (left > right) {\n    return -1;\n  }\n  int mid = left + (right - left) / 2;\n  if (ve[mid] > target) {\n    return BSearch1(ve, target);\n  } else if (ve[mid] < target) {\n    return BSearch1(ve, target);\n  }\n  return ve[mid];\n}\n```\n\n## 1.2 基本的二分搜索（循环）\n\n```c++\n// 普通二分：循环\nint BSearch2(vector<int> ve, int target) {\n  int left = 0, right = ve.size()-1;\n  while (left <= right) {\n    int mid = left + (right - left) / 2;\n    if (ve[mid] > target) {\n      right = mid - 1;\n    } else if (ve[mid] < target) {\n      left = mid + 1;\n    } else {\n      return ve[mid];\n    }\n  }\n}\n```\n\n## 2.1 查找第一个与target相等的元素的索引\n\n```c++\nint BSearch3(vector<int> ve, int target) {\n  int left = 0, right = ve.size()-1;\n  while (left <= right) {\n    int mid = left + (right - left) / 2;\n    if (ve[mid] >= target) {\n      right = mid - 1;\n    } else {\n      left = mid + 1;\n    }\n  }\n  return ve[left] == target ? left : -1;\n}\n```\n\n## 2.2 查找最后一个与target相等的元素的索引\n\n```c++\nint BSearch4(vector<int> ve, int target) {\n  int left = 0, right = ve.size()-1;\n  while (left <= right) {\n    int mid = left + (right - left) / 2;\n    if (ve[mid] <= target) {\n      left = mid + 1;\n    } else {\n      right = mid - 1;\n    }\n  }\n  return ve[right] == target ? right : -1;\n}\n```\n\n## 2.3 查找最后一个小于target的元素索引\n\n```c++\nint BSearch5(vector<int> ve, int target) {\n  int left = 0, right = ve.size()-1;\n  while (left <= right) {\n    int mid = left + (right - left) / 2;\n    if (ve[mid] < target) {\n      left = mid + 1;\n    } else {\n      right = mid - 1;\n    }\n  }\n  return right;\n}\n\n```\n\n## 2.4 查找第一个大于target的元素索引\n\n```c++\nint BSearch6(vector<int> ve, int target) {\n  int left = 0, right = ve.size()-1;\n  while (left <= right) {\n    int mid = left + (right - left) / 2;\n    if (ve[mid] > target) {\n      right = mid - 1;\n    } else {\n      left = mid + 1;\n    }\n  }\n  return left;\n}\n```\n\n## 2.5 查找最后一个小于等于target的元素的索引\n\n```c++\nint BSearch7(vector<int> ve, int target) {\n  int left = 0, right = ve.size()-1;\n  while (left <= right) {\n    int mid = left + (right - left) / 2;\n    if (ve[mid] <= target) {\n      left = mid + 1;\n    } else {\n      right = mid - 1;\n    }\n  }\n  return right;\n}\n```\n\n## 2.6 查找第一个大于等于target的元素的索引\n\n```c++\nint BSearch8(vector<int> ve, int target) {\n  int left = 0, right = ve.size()-1;\n  while (left <= right) {\n    int mid = left + (right - left) / 2;\n    if (ve[mid] >= target) {\n      right = mid - 1;\n    } else {\n      left = mid + 1;\n    }\n  }\n  return left;\n}\n```\n\n## 2.7 查找target所处范围\n\n```c++\n// 查找target所处范围，没有返回-1\nvector<int> BSearch9(vector<int> vex, int target) {\n  vector<int> res;\n  int leftIndex = BSearch3(vex, target);\n  int rightIndex = BSearch4(vex, target);\n  res.push_back(leftIndex < 0 ? -1 : leftIndex);\n  res.push_back(rightIndex < 0 ? -1 : rightIndex);\n  return res;\n}\n```\n\n","tags":["数据结构","C++"],"categories":["数据结构与算法","二分查找"]},{"title":"Vue生命周期解析","url":"/2018/06/10/Vue生命周期解析/","content":"\n\n## 简介\n**定义**：每个 `Vue` 实例在被创建之前都要经过一系列的初始化过程。例如需要设置数据监听、编译模板、挂载实例到 `DOM`、在数据变化时更新 `DOM` 等，不同的时期对应不同的周期；\n\n**生命周期函数：**不同周期开放出来的接口；\n\n![Vue生命周期](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/Vue生命周期/Vue生命周期图.png)\n\n**Vue的生命周期函数主要包括以下几个:**\n\n1. beforeCreate 、created \n2. beforeMount 、mounted\n3. beforeUpdate、  updated\n4. beforeDestroy、destroyed  \n\n\n\n## 流程解读\n\n**第一步：**初始化事件和生命周期。此时\\$data、\\$el和message均处于undefined状态。（前缀 `$`，以便与用户定义的属性区分开来）。\n\n**beforeCreate**：此时组件实例未创建，各个属性均没有生成。\n\n**第二步：**Init、injections、 reactivity。属性均已注入绑定，而且被`$watch`变成`reactivity`。但是`$el`还是没有生成，也就是`DOM`没有生成；\n\n**created：**\\$data和message均已存在，\\$el还没有。\n\n**第三步：**判断vue实例中是否有\\$el。如果有，则判断是否有template。如果没有则在手动挂载\\$el之后，再判断是否有template。\n\n**第四步：**\n\n1、在实例内部有template属性的时候，直接用内部的，然后调用render函数去渲染。 \n\n2、在实例内部没有找到template，就调用外部的html。实例内部的template属性比外部的优先级高。\n\n3、要是前两者都不满足，那么就抛出错误。\n\n**beforeMount：**只编译了模板，并没有挂载属性。即此时存在的还是虚拟DOM\n\n**第五步：**创建vm.$el替换虚拟DOM（vm为初始化的实例对象）。\n\n**mounted：**此时属性已挂载。\\$data、\\$el和message均处于已定义的状态。\n\n\n\n### beforeUpdate和updated：\n\n当数据改变时，这两个生命周期函数控制view层重新渲染。\n\n**渲染步骤：**数据改变——导致虚拟DOM的改变——调用这两个生命钩子去改变视图\n\n**1、**只有当数据与模板中的数据绑定才会这两个函数才会有效；\n\n```javascript\nvar vm = new Vue({\n  el: '#app',\n  template: '<div id=\"app\"></div>',  // 这里需要是<div id=\"app\">{{a}}</div>才有效果\n  beforeUpdate: function() {\n    console.log('调用了beforeUpdate')\n  },\n  updated: function() {\n    console.log('调用了uodated')\n  },\n  data: {\n    a: 1\n  }\n})\n\nvm.a = 2 // 虽然数据发生了改变，但是并未与模板绑定，所以控制台不会打印任何一条语句\n```\n\n**2、**数据改变后，在beforeUpdate和updated中分别`console.log(this.$el)`发现输出结果相同\n\n**beforeUpdate：**数据更新时调用，发生在虚拟 DOM 重新渲染和打补丁之前。\n**updated：**由于数据更改导致的虚拟 DOM 重新渲染和打补丁，在这之后会调用该钩子。\n\nthis.$el是一个对象，或者说是一个指针。所以更新之后显示的都是一样的。可以通过\n\n```javascript\nconsole.log(\"真实的DOM结构: \"+ document.getElementById('app').innerHTML)\n```\n\n观察真实的DOM结构比对。实际中可以发现beforeUpdate中还是原来的数据，updated变成了之后的数据。\n\n\n\n### beforeDestory和destoryed：\n\n1、使用`app.$destroy()`进行销毁；\n\n销毁后DOM元素仍然存在，但是再次改变data的值，beforeUpdate和updated均不起作用。即`Vue` 实例指示的所有东西都会解绑定，所有的事件监听器会被移除，所有的子实例也会被销毁\n\n\n\n## 生命周期函数的使用场景：\n\n1. beforeCreate : 举个栗子：可以在这加个loading事件 \n2. created ：在这结束loading，还做一些初始化，实现函数自执行 \n3. mounted ： 在这发起后端请求，拿回数据，配合路由钩子做一些事情\n4. beforeDestroy： 你确认删除XX吗？ destroyed ：当前组件已被删除，清空相关内容\n\n\n\n\n\n## 示例代码：（监测\\$el、\\$data、\\$message变化）\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\">\n  <title>vue生命周期学习</title>\n  <script src=\"https://cdn.jsdelivr.net/npm/vue@2.5.17/dist/vue.js\"></script>\n</head>\n<body>\n  <div id=\"app\">\n    <h1>{{message}}</h1>\n  </div>\n</body>\n<script>\n  var vm = new Vue({\n    el: '#app',\n    data: {\n      message: 'Vue的生命周期'\n    },\n    // 模板将会 替换 挂载的元素\n    beforeCreate: function() {\n      console.group('------beforeCreate创建前状态------');\n      console.log(\"%c%s\", \"color:red\" , \"el     : \" + this.$el); //undefined\n      console.log(\"%c%s\", \"color:red\",\"data   : \" + this.$data); //undefined \n      console.log(\"%c%s\", \"color:red\",\"message: \" + this.message) \n    },\n    created: function() {\n      console.group('------created创建完毕状态------');\n      console.log(\"%c%s\", \"color:red\",\"el     : \" + this.$el); //undefined\n      console.log(\"%c%s\", \"color:red\",\"data   : \" + this.$data); //已被初始化 \n      console.log(\"%c%s\", \"color:red\",\"message: \" + this.message); //已被初始化\n    },\n    beforeMount: function() {\n      console.group('------beforeMount挂载前状态------');\n      console.log(\"%c%s\", \"color:red\",\"el     : \" + (this.$el)); //已被初始化\n      console.log(this.$el);\n      console.log(\"%c%s\", \"color:red\",\"data   : \" + this.$data); //已被初始化  \n      console.log(\"%c%s\", \"color:red\",\"message: \" + this.message); //已被初始化  \n    },\n    mounted: function() {\n      console.group('------mounted 挂载结束状态------');\n      console.log(\"%c%s\", \"color:red\",\"el     : \" + this.$el); //已被初始化\n      console.log(this.$el);\n      console.log(\"%c%s\", \"color:red\",\"data   : \" + this.$data); //已被初始化\n      console.log(\"%c%s\", \"color:red\",\"message: \" + this.message); //已被初始化 \n    },\n    beforeUpdate: function () {\n      console.group('beforeUpdate 更新前状态===============》');\n      console.log(\"真实的DOM结构: \"+ document.getElementById('app').innerHTML)\n      console.log(\"%c%s\", \"color:red\",\"el     : \" + this.$el);\n      console.log(this.$el);   \n      console.log(\"%c%s\", \"color:red\",\"data   : \" + this.$data); \n      console.log(\"%c%s\", \"color:red\",\"message: \" + this.message); \n    },\n    updated: function () {\n      console.group('updated 更新完成状态===============》');\n      console.log(\"真实的DOM结构: \"+ document.getElementById('app').innerHTML)\n      console.log(\"%c%s\", \"color:red\",\"el     : \" + this.$el);\n      console.log(this.$el); \n      console.log(\"%c%s\", \"color:red\",\"data   : \" + this.$data); \n      console.log(\"%c%s\", \"color:red\",\"message: \" + this.message); \n    },\n    beforeDestroy: function () {\n      console.group('beforeDestroy 销毁前状态===============》');\n      console.log(\"%c%s\", \"color:red\",\"el     : \" + this.$el);\n      console.log(this.$el);    \n      console.log(\"%c%s\", \"color:red\",\"data   : \" + this.$data); \n      console.log(\"%c%s\", \"color:red\",\"message: \" + this.message); \n    },\n    destroyed: function () {\n      console.group('destroyed 销毁完成状态===============》');\n      console.log(\"%c%s\", \"color:red\",\"el     : \" + this.$el);\n      console.log(this.$el);  \n      console.log(\"%c%s\", \"color:red\",\"data   : \" + this.$data); \n      console.log(\"%c%s\", \"color:red\",\"message: \" + this.message)\n    }\n  })\n</script>\n</html>\n```\n\n\n\n\n\n\n\n## 参考文献\n\n1. [详解vue生命周期 - SegmentFault思否](https://segmentfault.com/a/1190000011381906)\n2. [API - Vue.js](https://cn.vuejs.org/v2/api/#template)\n3. [vue生命周期详解 - 掘金](https://juejin.im/post/5afd7eb16fb9a07ac5605bb3)\n4. [vue2 为什么beforeUpdate时的$el 和$data与updated时的一样](https://segmentfault.com/q/1010000011521681)\n\n\n\n\n\n","tags":["Vue"],"categories":["Vue","基础原理类"]},{"title":"静态资源加速","url":"/2018/05/17/静态资源加速/","content":"<br />\n## 网站的静态资源介绍\n\n在使用浏览器控制台观测各个网站的详细信息时，可以看到`head`标签中存在大量引用的js、css或其它类型的文件。如下所示：\n``` html\n<!--指向站点内部的文件-->\n<meta content=\"/images/branding/googleg/1x/googleg_standard_color_128dp.png\" itemprop=\"image\">\n<!--绝对路径指向外部站点的js-->\n<script src=\"http://apps.bdimg.com/libs/bootstrap/3.3.0/js/bootstrap.min.js\"></script>\n```\n如果网站规模较小、资源请求数少、对存储空间、并发量的要求不高，可以尝试将资源代码与业务代码存放在一起的方式。如果网站规模扩大，优化的方式也有许多，单从静态资源的角度看可以将资源服务器与业务服务器分离。这就引入了CDN的概念\n<br />\n## CDN（Content Delivery Network）\nCDN即内容分发网络。通俗意义上说就是能够让你访问网站的速度变快。可以将资源类型的文件存储在CDN服务器中，CDN系统根据当前互联网运行的各项指标如节点的连接、负载、到用户的响应时间和距离，将请求导向离用户最近的服务节点上。这样既保证了访问的稳定性与速度也降低了业务服务器的资源流量。\n国内做CDN的厂商有阿里云、百度、七牛、又拍等。阿里云的售后保障是比较靠谱的，当然价格也相对较高，其他公司的产品没用过，不做评价。其中有一家叫白山云的公司，产品好不好不知道，但面试体验是极好的。（可惜没有谁给我广告费）\n<br />\n## 图片资源的处理\n静态站点中如果存在的大量图片，那么图片请求时间过长会导致站点的访问速度变慢。提高网站的图片加载速度当然也可以使用CDN，同时也可以对图片本身进行压缩合并。这里提供一种思路：\n本站的原始背景图片是大小为4M左右格式为png的图片。经压缩后大小为1M左右，格式仍为png，肉眼看不出变化，压缩率达到75%左右。压缩站点[TinyPNG](https://tinypng.com/)。但是实测中访问速度不是很明显。\n所以再次将图片压缩为webP格式，肉眼仍看不出变化，压缩度达到90%左右。压缩站点[Squoosh](https://squoosh.app/)。但是该图片格式由谷歌推出，Chrome与Opera能够实现支持，FireFox暂不可以（据说可能需要到明年上半年）。所以使用Firefox等浏览器访问得到的是`#e7e7e7`为背景色的网站。\n<br />\n\n## 参考文献\n\n1. [CDN是什么？使用CDN有什么优势？ - 知乎](https://www.zhihu.com/question/36514327)\n2. [WebP 相对于 PNG、JPG 有什么优势？- 知乎](https://www.zhihu.com/question/27201061)\n3. [网站加载图片速度慢如何优化 - SegmentFault 思否](https://segmentfault.com/q/1010000006201412)\n4. [webP - 维基百科](https://zh.wikipedia.org/wiki/WebP)\n<br />\n","tags":["CDN"],"categories":["建站实录","静态资源加速"]},{"title":"二叉树的中序遍历","url":"/2018/04/24/二叉树中序遍历/","content":"\n<br />\n## 方法一：递归方式\n\n```C++\nclass Solution {\npublic:\n  vector<int> res;\n  vector<int> inorderTraversal(TreeNode* root) {\n    if (root) {\n      inorderTraversal(root->left);\n      res.push_back(root->val);\n      inorderTraversal(root->right);\n    }\n    return res;  \n  }\n};\n```\n\n## 方法二：非递归（栈1）\n该方式与前序遍历中[方法三]()相似。只不过在逐步遍历左子节点的过程中并不记录遍历顺序，而是在栈中取出的时候记录。并将游标节点指向当前从栈中取出节点的右子节点。把他当成根节点继续遍历。\n```C++\nclass Solution {\npublic:\n  vector<int> res;\n  vector<int> inorderTraversal(TreeNode* root) {\n    vector<int> res;\n    if (!root) {\n      return res;\n    }\n    stack<TreeNode*> s;\n    TreeNode* node = root;\n    while(!s.empty() || node) {\n      if (node) {\n        s.push(node);\n        node = node->left;\n      } else {\n        TreeNode* temp = s.top();\n        s.pop();\n        res.push_back(temp->val);\n        node = temp->right;\n      }\n    }\n    return res;  \n  }\n};\n```\n\n## 方法三：非递归\n方法三利用了一个游标节点和一个前驱节点。按照中序遍历的顺序，前驱节点是游标节点的上一个节点。\n沿着左子树遍历，如果左子树不存在，则将当前游标节点导入列表。如果存在则将当前游标节点的左子树中序遍历的最后一个节点指向自身（假设还没有指向的情况下），见图中箭头。如果已经指向自身，那么将当前节点导入列表。并且游标指向当前节点的右子节点，继续循环直到游标节点指为空。\n\n![方法三图示](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/二叉树的中序遍历/二叉树中序遍历.png)\n\n```C++\nclass Solution {\npublic:\n  vector<int> inorderTraversal(TreeNode* root) {\n    vector<int> res;\n    if (!root) {\n      return res;\n    }\n    TreeNode* pre;\n    TreeNode* cur = root;\n    while(cur) {\n      if (!cur->left) {\n        res.push_back(cur->val);\n        cur = cur->right;\n      } else {\n        pre = cur->left;\n        while(pre->right && pre->right != cur) {\n          pre = pre->right;\n        }\n        if (!pre->right) {\n          pre->right = cur;\n          cur = cur->left;\n        } else {\n          pre->right = NULL;\n          res.push_back(cur->val);\n          cur = cur->right;\n        }\n      }\n    }\n    return res;\n  }\n};\n```\n<br />\n","tags":["数据结构","C++","二叉树"],"categories":["数据结构与算法","二叉树"]},{"title":"二叉树的前序遍历","url":"/2018/04/15/二叉树前序遍历/","content":"\n<br />\n## 方法一：递归方式\n\n```c++\nclass Solution {\npublic:\n  vector<int> res;\n  vector<int> preorderTraversal(TreeNode* root) {\n    if (root) {\n      res.push_back(root->val);\n      preorderTraversal(root->left); \n      preorderTraversal(root->right);\n    }\n    return res;\n  }\n};\n```\n\n## 方法二：非递归（栈1）\n\n首先将根节点入栈。然后循环遍历，取出根节点，考虑到栈的**后进先出**原则，首先入栈右边节点，之后入栈左边节点。再次取出栈顶节点直到栈为空。\n\n```c++\nclass Solution {\npublic:\n  vector<int> preorderTraversal(TreeNode* root) {\n    vector<int> res;\n    if (!root) {\n      return res;\n    }\n    stack<TreeNode* > s{{root}};\n    while(!s.empty()) {\n      TreeNode* node = s.top();\n      res.push_back(node->val);\n      s.pop();\n      if (node->right) {\n        s.push(node->right);\n      }\n      if (node->left) {\n        s.push(node->left);\n      }\n    }\n    return res;\n  }\n};\n```\n\n## 方法三：非递归（栈2）\n\n1、新建栈并且设置一个游标节点，游标节点走过的路径就是二叉树的前序遍历顺序。\n\n2、首先循环条件是栈不为空且游标节点存在。游标节点首先为根节点。\n\n3、游标节点沿着自身左子树遍历，将遍历顺序存到列表当中，并将遍历到的节点逐个入栈，直到左子节点不存在。\n\n4、取出栈顶元素的右子节点作为游标节点，因为栈顶元素在初始入栈的过程中已经被记录，不必重复记录。（之后做的实际上是将此时的游标节点当做根节点再次执行以上操作）。\n\n5、直到栈为空且游标节点不存在。\n\n```c++\nclass Solution {\npublic:\n  vector<int> preorderTraversal(TreeNode* root) {\n    vector<int> res; \n    if (!root) {\n      return res;\n    }\n    stack<TreeNode*> s;\n    TreeNode* node = root;\n    while (!s.empty() || node) {\n      if (node) {\n        res.push_back(node->val);\n        s.push(node);\n        node = node->left;\n      } else {\n        TreeNode* t = s.top();\n        s.pop();\n        node = t->right;\n      }\n    }\n    return res;\n  }\n};\n```\n<br />\n","tags":["数据结构","C++","二叉树"],"categories":["数据结构与算法","二叉树"]}]