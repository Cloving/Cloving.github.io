{"pages":[{"title":"关于我","text":"基本信息 Browser: &nbsp; Search Engine：&nbsp; Plantform: &nbsp; &nbsp;&nbsp; Editor: &nbsp; Visual Studio Code &nbsp;&nbsp; Jupyter Notebook &nbsp;&nbsp; Pycharm Fonts: &nbsp; Dejavu Sans Mono &nbsp;&nbsp; Monaco &nbsp;&nbsp; Roboto Indentation: &nbsp; 2 Spaces Writing: &nbsp; Typora Music: &nbsp; Netease cloud Player: &nbsp; PotPlayer Hobby: &nbsp; Meteorological &nbsp;&nbsp; Rail Transit 足迹 // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(document.getElementById('echarts2456')); // 指定图表的配置项和数据 var data = [ {name: '成都', value: 200},{name: '西安', value: 200}, {name: '青城山', value: 120},{name: '都江堰', value: 120}, {name: '重庆', value: 200},{name: '长沙', value: 200}, {name: '华山', value: 120},{name: '平遥', value: 120}, {name: '灵石', value: 120},{name: '徐州', value: 120}, {name: '宿迁', value: 120},{name: '上海', value: 200}, {name: '南京', value: 200},{name: '滁州', value: 120}, {name: '淮南', value: 120},{name: '蚌埠', value: 120}, {name: '合肥', value: 200},{name: '中庙', value: 120}, {name: '芜湖', value: 120},{name: '广州', value: 200}, {name: '顺德', value: 120}, ]; var geoCoordMap = { '成都':[104.06,30.67],'西安':[108.95,34.27],'青城山':[103.57,30.90], '都江堰':[103.61,30.98],'重庆':[106.56,29.56],'长沙':[113,28.21], '华山':[110.08,34.47],'平遥':[112.18,37.20],'灵石':[111.77,36.88], '徐州':[117.2,34.26],'宿迁':[118.3,33.96],'上海':[121.48,31.22], '南京':[118.78,32.04],'滁州':[118.31,32.31],'淮南':[117.01,32.64], '蚌埠':[117.37,32.92],'合肥':[117.27,31.86],'中庙':[117.47,31.59], '芜湖':[118.38,31.33],'广州':[113.23,23.16],'顺德':[113.28,22.81], }; var convertData = function (data) { var res = []; for (var i = 0; i < data.length; i++) { var geoCoord = geoCoordMap[data[i].name]; if (geoCoord) { res.push({ name: data[i].name, value: geoCoord.concat(data[i].value) }); } } return res; }; option = { title : { text: '', subtext: '', left: 'center', top: 'top', textStyle: { color: '#fff' } }, tooltip: {}, backgroundColor: { type: 'linear', x: 0, y: 0, x2: 1, y2: 1, colorStops: [ { offset: 0, color: '#F0F8FF' // 0% 处的颜色 }, { offset: 1, color: '#091732' // 100% 处的颜色 } ], globalCoord: false // 缺省为 false }, geo: { map: 'china', show: true, roam: true, label: { emphasis: { show: false } }, itemStyle: { normal: { areaColor: '#D3D3D3', borderColor: '#3B5077', // shadowColor: '#1773c3', // shadowBlur: 20 }, emphasis: { areaColor: '#3CB371', } } }, series: [ { name: '', type: 'effectScatter', coordinateSystem: 'geo', data: convertData(data), symbolSize: function (val) { return val[2] / 20; }, showEffectOn: 'render', rippleEffect: { brushType: 'stroke' }, hoverAnimation: true, label: { normal: { formatter: '{b}', position: 'right', show: false } }, itemStyle: { normal: { color: '#ff4500', shadowBlur: 10, shadowColor: '#333' } }, zlevel: 1 } ] }; // 使用刚指定的配置项和数据显示图表。 myChart.setOption(option);","link":"/about/index.html"},{"title":"","text":"layout: tags","link":"/tags/index.html"},{"title":"","text":"layout: categories","link":"/categories/index.html"}],"posts":[{"title":"Echarts足迹图","text":"摘要Echarts是百度EFE可视化团队开发的基于Javascript开源可视化图表库。由于部分blog文章中有展示图表的需求，基于此，本文研究了如何将Echarts的图表嵌入到blog中去。 Hexo插件hexo-tag-echarts3是一款在Hexo博客中导入echarts图表的插件。但是它不支持导入地图，同时该插件要求所有渲染的代码都必须在option的内部（变量option一般用于指定了图表的配置项和数据）。如果有调用外部函数、外部变量的情况，效果可能就不太好。所以对插件进行了修改，支持了地图功能，同时改进了渲染机制，即渲染代码只含有option，或者除option外还调用了外部的函数的情况都可以成功执行。 插件修改引入地图首先需要修改引入的echarts文件，因为echarts通用包中不含有地图组件，所以必须使用完整包或者在线定制的方式1&lt;script type=\"text/javascript\" src=\"https://cdn.bootcss.com/echarts/4.2.0-rc.2/echarts.min.js\"&gt;&lt;/script&gt; 另Echarts现在不能够直接下载地图，所以需要能够找到外部地图js文件。可以从github上下载China.js再从本地导入。例如：下载到/themes/主题名/source/js/中，导入方式为：1&lt;script type=\"text/javascript\" src=\"/js/echarts.js\"&gt;&lt;/script&gt; 或者引用线上文件：1&lt;script type=\"text/javascript\" src=\"http://gallery.echartsjs.com/dep/echarts/map/js/china.js\"&gt;&lt;/script&gt; 注意：根据&lt;script&gt;加载机制，引用顺序必须先引入echarts.js，后载入地图js文件 修改渲染方式修改模板文件template.html代码12345678&lt;script type=\"text/javascript\"&gt; // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(document.getElementById('&lt;%- id %&gt;')); // 指定图表的配置项和数据 &lt;%= sourceCode %&gt; // 使用刚指定的配置项和数据显示图表。 myChart.setOption(option);&lt;/script&gt; 修改index.js文件12345678910111213function echartsMaps(args, content) { var template = fs.readFileSync(filePath).toString(), options = {}; if (content.length) { var options = content; } return _.template(template)({ id: 'echarts' + ((Math.random() * 9999) | 0), sourceCode: content, height: args[0] || 400, width: args[1] || '85%' });} 其中参数content为嵌入的代码，以上修改会将嵌入的代码直接渲染，之后调用myChart.setOption(option);使用配置项和数据显示图表。 渲染图 总结本文叙述了如何将Echarts的图表嵌入到blog中，由于我需要使用足迹地图并且会调用option外部的函数，所以修改了原来的插件。修改后的插件地址：hexo-tag-echarts-chart，欢迎试用。 参考文献1、使用 ECharts3.0 在 Hexo 搭建的博客中建一个足迹👣页面 | Docle の Blog2、在 Hexo 中插入 ECharts 动态图表 - KChen’s Blog3、谈谈script标签以及其加载顺序问题，包含 defer async - 个人文章 - SegmentFault 思否4、浏览器加载 JS 文件的先后顺序同具体的解析和执行有什么关系？ - 知乎5、kchen0x/hexo-tag-echarts3: A simple plugin for inserting ECharts 3 by using tags in Hexo","link":"/2018/12/08/Echarts足迹图/"},{"title":"三寸气在千般用，一旦无常万事休","text":"其实人生很长，选择一个自己真正想要从事的领域，沉淀下去，把自己变得更厉害一点。因为对于人来说不怕有缺点，就怕没特点。但不要想着一定要比谁谁更好，因为不论你多努力，总有人比你厉害，也总有人永远不如你。和别人比较大概是生而为人最可悲的事。总而言之，不应当有太多的欲望，不要做了一件事就必须要得到什么。但行好事，莫问前程。 其实人生也很短，生命是脆弱的，一次看似普通的熬夜也许就榨干了你全部的精力，一次看似普通的疾病也许会换来了一张薄薄的病危通知书。珍视自己，珍视珍视你的人，不做让自己后悔的事。","link":"/2018/09/21/三寸气在千般用，一旦无常万事休/"},{"title":"Python模拟豆瓣登录（一）","text":"摘要抓取数据的过程中有时候需要完成模拟登录的操作，本文使用requests库完成豆瓣的模拟登录，并保存已登录的Cookie，方便下次直接登录。之后通过访问个人主页验证当前状态是否为已登录状态。 requests请求分析过程首先需要研究豆瓣的登录机制，在豆瓣的登录界面查看登录请求提交的表单项，如图所示： 图中的Login请求为post类型，Form Data表示post到服务器的数据，可以看出数据并未加密。 1、source表示该登录页面是由豆瓣读书跳转过来2、redir表示登录后跳转到的url3、form_email和form_password分别表示用户名和密码4、captcha-solution则是验证码5、captcha-id验证码的id该字段可从登录页面的HTML中获取 编码过程1、初始化变量：123456789101112131415161718192021def __init__(self): self.session = requests.Session() self.session.cookies = cookielib.LWPCookieJar(filename=\"cookies.txt\") self.url = 'https://accounts.douban.com/login' # 登录url self.redirurl = 'https://book.douban.com/mine' # 重定向url self.email = '******' self.password = '******' # 构造post数据 self.data = { 'redir': self.redirurl, 'form_email': self.email, 'form_password': self.password, 'login': '登录' } # 构造用户代理 self.headers = { 'User-Agent': 'Mozilla/5.0 ' '(Windows NT 10.0; Win64; x64) ' 'AppleWebKit/537.36 (KHTML, like Gecko) ' 'Chrome/55.0.2883.87 Safari/537.36' } 2、发送请求我们模拟登录需要的是登录之后的页面，所以利用Session来维持一种会话状态，并且保存登录后的Cookie。下次登录时直接携带Cookie发送请求，无需再使用账号密码。判断登录时是否需要验证码先使用BeautifulSoup库来获取登录页面的HTML代码，之后利用正则表达式判断其中是否有显示验证码的img标签，如果有处理验证码，如果没有那么就将已有的信息post到服务器，并保存登录后的Cookie。 12345678910111213def login(self): page = self.session.post(self.url, headers=self.headers) soup = BeautifulSoup(page.text, \"html.parser\") captcha = soup.find('img', id='captcha_image') if captcha is not None: self.process_captcha(page, captcha) afterLogin_page = self.session.post(self.url, data=self.data, headers=self.headers) else: afterLogin_page = self.session.post(self.url, data=self.data, headers=self.headers) # print(self.session.cookies) self.session.cookies.save(ignore_discard=True, ignore_expires=True) print(afterLogin_page.text) soup = BeautifulSoup(afterLogin_page.text, \"html.parser\") 关于Cookie与SessionHttp是一种无状态的协议，因此并不能追踪用户的状态，所以常采用Session与Cookie结合的方式跟踪用户的状态。Session位于服务器端只保存对话信息但是不能够识别出具体的用户，Cookie位于客户端用于存放用户信息。Cookie在登录网站时会由服务器产生传递给客户端，当客户端再次登录时会携带Cookie，之后服务器便根据Cookie中的Session ID跟踪到会话。如果会话有效，那么会判断用户已处于登录状态，否则可能会判断用户没有访问权限进而跳转到登录页面。同时Cookie是有过期时间的，超过该时间Cookie失效，需要重新获取。这同时也是为了避免Cookie被他人获取并长期使用。 3、处理验证码这里先使用手动输入验证码的方式，首先拿到验证码图片的地址（这个地址是临时的，一段时间后会失效，但对于手动输入并不影响），然后利用正则表达式拿到captcha-id，因为它需要与验证码一同post到服务器。具体代码如下： 123456789101112131415161718def process_captcha(self, page, captcha): # 处理验证码 # 获得验证码图片地址 captcha_url = captcha['src'] # 利用正则表达式获得验证码ID pattern = re.compile('&lt;input type=\"hidden\" name=\"captcha-id\" value=\"(.*?)\"/') captcha_id = re.search(pattern, page.text).group(1) # 将验证码图片保存到本地 urllib.request.urlretrieve(captcha_url, \"captcha.png\") try: image = Image.open('captcha.png') image.show() image.close() except Exception as e: print(\"打开验证码图片失败，请手动重试\") captcha = input('please input the captcha:') self.data['captcha-solution'] = captcha self.data['captcha-id'] = captcha_id 4、利用Cookie登录载入本地Cookie之后再发送get请求到url，之后将请求到的HTML文本写入文件。注意：以二进制的类型写入需要bytes对象，所以使用utf8编码12345678910def get_index(self): # 根据本地cookies登录 try: self.session.cookies.load(ignore_discard=True) except Exception as e: print(\"cookie未能加载, 原因: \", e) response = self.session.get(self.redirurl, headers=self.headers) with open(\"index.html\", 'wb') as f: f.write(response.text.encode('utf8')) print(\"已载入本地Cookie\") 总结过程比较简单，编码过程也不复杂，验证码的处理暂时使用手动的方式。关于自动识别豆瓣验证码的方式正在研究中，目前效果还不理想。完整代码：douban_login_1.py 参考文献1、python 爬虫 cookie 的保存和加载 - 盖娅 - 开源中国2、Python爬虫基础练习(十一)简单模拟豆瓣登录 - 知乎","link":"/2018/11/30/Python模拟豆瓣登录（一）/"},{"title":"二分搜索算法","text":"1.1 基本的二分搜索(递归)1234567891011121314// 普通二分：递归int BSearch1(vector&lt;int&gt; ve, int target) { int left = 0, right = ve.size()-1; if (left &gt; right) { return -1; } int mid = left + (right - left) / 2; if (ve[mid] &gt; target) { return BSearch1(ve, target); } else if (ve[mid] &lt; target) { return BSearch1(ve, target); } return ve[mid];} 1.2 基本的二分搜索（循环）1234567891011121314// 普通二分：循环int BSearch2(vector&lt;int&gt; ve, int target) { int left = 0, right = ve.size()-1; while (left &lt;= right) { int mid = left + (right - left) / 2; if (ve[mid] &gt; target) { right = mid - 1; } else if (ve[mid] &lt; target) { left = mid + 1; } else { return ve[mid]; } }} 2.1 查找第一个与target相等的元素的索引123456789101112int BSearch3(vector&lt;int&gt; ve, int target) { int left = 0, right = ve.size()-1; while (left &lt;= right) { int mid = left + (right - left) / 2; if (ve[mid] &gt;= target) { right = mid - 1; } else { left = mid + 1; } } return ve[left] == target ? left : -1;} 2.2 查找最后一个与target相等的元素的索引123456789101112int BSearch4(vector&lt;int&gt; ve, int target) { int left = 0, right = ve.size()-1; while (left &lt;= right) { int mid = left + (right - left) / 2; if (ve[mid] &lt;= target) { left = mid + 1; } else { right = mid - 1; } } return ve[right] == target ? right : -1;} 2.3 查找最后一个小于target的元素索引123456789101112int BSearch5(vector&lt;int&gt; ve, int target) { int left = 0, right = ve.size()-1; while (left &lt;= right) { int mid = left + (right - left) / 2; if (ve[mid] &lt; target) { left = mid + 1; } else { right = mid - 1; } } return right;} 2.4 查找第一个大于target的元素索引123456789101112int BSearch6(vector&lt;int&gt; ve, int target) { int left = 0, right = ve.size()-1; while (left &lt;= right) { int mid = left + (right - left) / 2; if (ve[mid] &gt; target) { right = mid - 1; } else { left = mid + 1; } } return left;} 2.5 查找最后一个小于等于target的元素的索引123456789101112int BSearch7(vector&lt;int&gt; ve, int target) { int left = 0, right = ve.size()-1; while (left &lt;= right) { int mid = left + (right - left) / 2; if (ve[mid] &lt;= target) { left = mid + 1; } else { right = mid - 1; } } return right;} 2.6 查找第一个大于等于target的元素的索引123456789101112int BSearch8(vector&lt;int&gt; ve, int target) { int left = 0, right = ve.size()-1; while (left &lt;= right) { int mid = left + (right - left) / 2; if (ve[mid] &gt;= target) { right = mid - 1; } else { left = mid + 1; } } return left;} 2.7 查找target所处范围123456789// 查找target所处范围，没有返回-1vector&lt;int&gt; BSearch9(vector&lt;int&gt; vex, int target) { vector&lt;int&gt; res; int leftIndex = BSearch3(vex, target); int rightIndex = BSearch4(vex, target); res.push_back(leftIndex &lt; 0 ? -1 : leftIndex); res.push_back(rightIndex &lt; 0 ? -1 : rightIndex); return res;}","link":"/2018/06/13/二分搜索算法/"},{"title":"二叉树的中序遍历","text":"方法一：递归方式123456789101112class Solution {public: vector&lt;int&gt; res; vector&lt;int&gt; inorderTraversal(TreeNode* root) { if (root) { inorderTraversal(root-&gt;left); res.push_back(root-&gt;val); inorderTraversal(root-&gt;right); } return res; }}; 方法二：非递归（栈1）该方式与前序遍历中方法三相似。只不过在逐步遍历左子节点的过程中并不记录遍历顺序，而是在栈中取出的时候记录。并将游标节点指向当前从栈中取出节点的右子节点。把他当成根节点继续遍历。123456789101112131415161718192021222324class Solution {public: vector&lt;int&gt; res; vector&lt;int&gt; inorderTraversal(TreeNode* root) { vector&lt;int&gt; res; if (!root) { return res; } stack&lt;TreeNode*&gt; s; TreeNode* node = root; while(!s.empty() || node) { if (node) { s.push(node); node = node-&gt;left; } else { TreeNode* temp = s.top(); s.pop(); res.push_back(temp-&gt;val); node = temp-&gt;right; } } return res; }}; 方法三：非递归方法三利用了一个游标节点和一个前驱节点。按照中序遍历的顺序，前驱节点是游标节点的上一个节点。沿着左子树遍历，如果左子树不存在，则将当前游标节点导入列表。如果存在则将当前游标节点的左子树中序遍历的最后一个节点指向自身（假设还没有指向的情况下），见图中箭头。如果已经指向自身，那么将当前节点导入列表。并且游标指向当前节点的右子节点，继续循环直到游标节点指为空。 12345678910111213141516171819202122232425262728293031class Solution {public: vector&lt;int&gt; inorderTraversal(TreeNode* root) { vector&lt;int&gt; res; if (!root) { return res; } TreeNode* pre; TreeNode* cur = root; while(cur) { if (!cur-&gt;left) { res.push_back(cur-&gt;val); cur = cur-&gt;right; } else { pre = cur-&gt;left; while(pre-&gt;right &amp;&amp; pre-&gt;right != cur) { pre = pre-&gt;right; } if (!pre-&gt;right) { pre-&gt;right = cur; cur = cur-&gt;left; } else { pre-&gt;right = NULL; res.push_back(cur-&gt;val); cur = cur-&gt;right; } } } return res; }};","link":"/2018/04/24/二叉树中序遍历/"},{"title":"二叉树的前序遍历","text":"方法一：递归方式123456789101112class Solution {public: vector&lt;int&gt; res; vector&lt;int&gt; preorderTraversal(TreeNode* root) { if (root) { res.push_back(root-&gt;val); preorderTraversal(root-&gt;left); preorderTraversal(root-&gt;right); } return res; }}; 方法二：非递归（栈1）首先将根节点入栈。然后循环遍历，取出根节点，考虑到栈的后进先出原则，首先入栈右边节点，之后入栈左边节点。再次取出栈顶节点直到栈为空。 12345678910111213141516171819202122class Solution {public: vector&lt;int&gt; preorderTraversal(TreeNode* root) { vector&lt;int&gt; res; if (!root) { return res; } stack&lt;TreeNode* &gt; s{{root}}; while(!s.empty()) { TreeNode* node = s.top(); res.push_back(node-&gt;val); s.pop(); if (node-&gt;right) { s.push(node-&gt;right); } if (node-&gt;left) { s.push(node-&gt;left); } } return res; }}; 方法三：非递归（栈2）1、新建栈并且设置一个游标节点，游标节点走过的路径就是二叉树的前序遍历顺序。 2、首先循环条件是栈不为空且游标节点存在。游标节点首先为根节点。 3、游标节点沿着自身左子树遍历，将遍历顺序存到列表当中，并将遍历到的节点逐个入栈，直到左子节点不存在。 4、取出栈顶元素的右子节点作为游标节点，因为栈顶元素在初始入栈的过程中已经被记录，不必重复记录。（之后做的实际上是将此时的游标节点当做根节点再次执行以上操作）。 5、直到栈为空且游标节点不存在。 1234567891011121314151617181920212223class Solution {public: vector&lt;int&gt; preorderTraversal(TreeNode* root) { vector&lt;int&gt; res; if (!root) { return res; } stack&lt;TreeNode*&gt; s; TreeNode* node = root; while (!s.empty() || node) { if (node) { res.push_back(node-&gt;val); s.push(node); node = node-&gt;left; } else { TreeNode* t = s.top(); s.pop(); node = t-&gt;right; } } return res; }};","link":"/2018/04/15/二叉树前序遍历/"},{"title":"抓取知乎用户动态数据","text":"一、简介打开知乎某个用户的主页可以看到该用户赞同回答、关注专栏、关注问题等行为数据。本文利用Python的requests库抓取知乎用户行为数据并存入MongoDB数据库中。 二、分析数据加载方式 打开网页后向下拉取可以观察到行为数据并不是一次加载完，而是随着用户的浏览进度逐步加载，考虑可能是ajax的形式。 之后打开Chrome控制台分析网络选项，查看XHR(XMLHttpRequest)类型的文件可以观察出收到的响应是json数据，其中包含了用户行为数据的相关信息 所以首先需要请求到网站返回的json数据，然后对数据进行解析，结构化处理，存入MongoDB中。 三、代码分析1、初始化变量1234567891011121314151617181920212223def __init__(self): self.user_name = \"******\" self.max_search_counts = ** self.params = { \"limit\": 7, \"desktop\": \"True\" } self.base_url = \"https://www.zhihu.com/api/v4/members/\" self.url = self.base_url + '/' + self.user_name + '/activities?' + urlencode(self.params) self.headers = { \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64)\\ AppleWebKit/537.36 (KHTML, like Gecko)\\ Chrome/70.0.3538.77 Safari/537.36\", \"Referer\": \"https://www.zhihu.com/\", \"x-requested-with\": \"fetch\" } self.client = MongoClient('127.0.0.1',username='admin', password='******', authSource='admin', authMechanism='SCRAM-SHA-1') self.db = self.client[\"zhihu\"] self.collection = self.db[\"zhihuDynamic\"] 1、user_name并不是用户的知乎ID，而是用户的个性化域名。比如知乎小管家的个性化域名为zhihuadmin，这可以通过观察用户主页的url获得2、请求的url就是返回json数据，在控制台的Network选项中观察到返回json数据的url类似于以下形式：1https://www.zhihu.com/api/v4/members/zhihuadmin/activities?limit=7&amp;after_id=1542365521&amp;desktop=True 分析组成可以看出https://www.zhihu.com/api/v4/members/为通用部分。zhihuadmin为用户的个性化域名。?之后是携带的参数3、抓取知乎的数据必须携带用户代理。 2、开始抓取123456789101112def startSearch(self): for search_times in range(0, self.max_search_counts): json = self.get_Page(self.url) try: self.url = json.get(\"paging\").get(\"next\") print(self.url) except Exception as e: print(\"获取下一次请求的链接失败，失败原因：\", e) results = self.get_Parse(json) print(\"=======================================\") for result in results: self.save_to_mongodb(result) for循环控制搜索次数，每次搜索会抓取7条数据即7次动态。之后提取出本次抓取到json数据中的用户动态信息，并更新下一次搜索的url，并将提取出的数据存入MongoDB中 3、抓取json数据12345678# 获取json数据def get_Page(self, url): try: response = requests.get(url, headers=self.headers) if response.status_code == 200: return response.json() except requests.ConnectionError as e: print(\"Error: \", e.args) 传入页面的url,返回相应的json数据 4、解析json数据根据传入json数据的格式，提取用户的各项动态 1234567891011121314151617181920212223# 解析json数据def get_Parse(self, json): if json: items = json.get('data') for item in items: zhihu = {} action_text = item.get(\"action_text\") created_time = item.get(\"created_time\") zhihu[\"操作行为\"] = action_text zhihu[\"时间\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(created_time)) if item.get('verb') == \"MEMBER_VOTEUP_ARTICLE\": item_detail = voteup_article(item.get(\"target\")) elif item.get('verb') == \"ANSWER_VOTE_UP\": item_detail = answer_voteUp(item.get(\"target\")) elif item.get('verb') == \"MEMBER_CREATE_ARTICLE\": item_detail = create_article(item.get(\"target\")) elif item.get('verb') == \"MEMBER_FOLLOW_COLUMN\": item_detail = follow_column(item.get('target')) elif item.get('verb') == \"ANSWER_CREATE\": item_detail = answer_create(item.get('target')) elif item.get('verb') == \"QUESTION_FOLLOW\": item_detail = question_follow(item.get('target')) yield dict(zhihu, **item_detail) 5、将解析出来的数据导入到数据库中1234# 导入到json数据库中def save_to_mongodb(self, result): if self.collection.insert_one(result): print(\"Successful save to Mongodb\") 四、总结代码写的并不好，仅作为参考。完整代码: 点这里 参考文献1、Authentication Examples — PyMongo 3.7.2 documentation2、Ajax结果提取 — Python3网络爬虫开发实战3、知乎小管家 — 知乎","link":"/2018/11/20/抓取知乎用户动态数据/"},{"title":"抓取知乎用户粉丝数据","text":"摘要本文抓取并分析了知乎用户的粉丝数据，包括粉丝名、粉丝标题、粉丝男女比例、粉丝回答问题比例、粉丝被关注数等数据。之后利用pyecharts库对这些数据进行可视化显示。 抓取数据1、请求的url与代理与之前一篇文章抓取知乎用户动态数据类似，这里请求的url也是返回json数据，所以同样在控制台network选项下找到请求的url，通过观察多个请求的url将其分为通用部分与扩展部分，如下形式：1234self.base_url = \"https://www.zhihu.com/api/v4/members/{user}/followers?{include}\"self.include = 'include=data%5B*%5D.answer_count%2Carticles_count%2C\\ gender%2Cfollower_count%2Cis_followed%2Cis_following%2C\\ badge%5B%3F(type%3Dbest_answerer)%5D.topics&amp;offset={offset}&amp;limit=20' 其中user为用户的个性化域名，include为扩展部分的url，offset为偏移量，即从第offset条数据开始。代理使用fake-useragent库，每次请求都随机生成，保证每次请求的header都不一样。 2、设置循环抓取首先根据url请求json数据，之后处理请求到的数据，得到一部分粉丝的信息，如果粉丝信息没有抓取完，则对抓取的url进行更新，直到数据抓取完成，结束的标志会在请求的json数据中获取到。最后对提取到的所有数据进行保存。代码如下：123456789101112131415161718192021def start(self, username, savepath, savename): index = 0 flag = True while flag: offset=index*20 include = self.include.format(offset=offset) url = self.base_url.format(user=username, include=include) # 请求json数据 print(\"正在进行第{}次请求\".format(index+1)) jsonData = self.requests_source(url) # 提取需要的数据 self.process_data(jsonData) # 如果仍然存在数据，处理下一次请求 if jsonData.get('paging').get('is_end') is False: index = index + 1 self.header['user-agent'] = self.agent.random else: flag = False print(\"完成请求\") # 保存数据 self.save_data(self.data, savepath, savename) 3、请求、处理数据请求处理的过程并不复杂，根据返回的状态码判断是否成功拿到json，之后会根据json的具体形式解析数据。1234567891011121314151617181920212223def requests_source(self, url): try: response = requests.get(url, headers=self.header, timeout=(5,30)) if response.status_code == 200: return response.json() except requests.ConnectionError as e: print(\"获取请求失败，失败原因：\", e)def process_data(self, jsonData): followers = jsonData.get('data') for follow in followers: name = self.read_data_from_jsonData(follow, 'name') headline = self.read_data_from_jsonData(follow, 'headline') gender = self.read_data_from_jsonData(follow, 'gender') follower_count = self.read_data_from_jsonData(follow, 'follower_count') answer_count = self.read_data_from_jsonData(follow, 'answer_count') self.data[name] = [headline, gender, follower_count, answer_count] print(name, self.data[name]) print('\\n')def read_data_from_jsonData(self, json, key): value = json.get(key) return value if value is not '' else 'Empty' 4、保存数据这里将所有信息保存到pkl文件当中1234567def save_data(self, follower_data, savepath, savename): if not os.path.exists(savepath): os.mkdir(savepath) with open(os.path.join(savepath, savename), 'wb') as f: pickle.dump(follower_data, f) f.close() print(\"完成存储\") 完成所有信息的抓取、处理及保存之后下一步就是解析可视化这些数据。 可视化数据可视化数据利用了pyecharts库与jieba库，前者用于可视化各类图表，后者是Python的中文分词组件。 1、打开pkl文件保存pkl文件的名称可以自行指定，每次抓取如果文件名称改变那么分析数据的时候需要修改代码，所以使用了正则匹配，这样无需关心pkl文件的名称就可以直接运行。代码如下：123456789101112for root, dirs, files in os.walk(os.path.join(os.path.dirname(__file__),'result')): # print(root) # 当前目录路径 # print(dirs) # 当前路径下所有子目录 # print(files) # 当前路径下所有非目录子文件 for file in files: try: file_name = re.search('(.*?).pkl', file).group() break except Exception as e: continuewith open('./result/'+file_name, 'rb') as f: followers_data = pickle.load(f) 2、提取数据为了便于分析、可视化数据，所以会将所有数据提取成[(元组1), (元组2),...,(元组3)]的形式，例如性别数据的形式为1[('男性': 32),('女性': 42),('保密': 52)] 生成headline与用户名的词云图时使用jieba库分词，其中在处理headline时需要删除掉一些符号字符之后再统计词频12345678910111213def statistic_word_frequency(self, texts, stopwords): # 统计词频 statistic_dict = {} for text in texts: temp = jieba.cut(text, cut_all=False) for t in temp: if t in stopwords or t == 'Empty': continue; if t in statistic_dict.keys(): statistic_dict[t] += 1 else: statistic_dict[t] = 1 return list(statistic_dict.items()) stopwords用来存储需要删除掉的一些字符。 3、示例图示例图是随机寻找的一位知乎用户进行分析的结果 总结本文的思路参考了知乎用户Charles的爬虫项目。最终完成的代码：知乎用户粉丝 参考文献1、Charles的皮卡丘2、pyecharts - A Python Echarts Plotting Library3、fxsjy/jieba: 结巴中文分词","link":"/2018/12/13/抓取知乎用户粉丝数据/"},{"title":"Vue生命周期解析","text":"简介定义：每个 Vue 实例在被创建之前都要经过一系列的初始化过程。例如需要设置数据监听、编译模板、挂载实例到 DOM、在数据变化时更新 DOM 等，不同的时期对应不同的周期； 生命周期函数：不同周期开放出来的接口； Vue的生命周期函数主要包括以下几个: beforeCreate 、created beforeMount 、mounted beforeUpdate、 updated beforeDestroy、destroyed 流程解读第一步：初始化事件和生命周期。此时\\$data、\\$el和message均处于undefined状态。（前缀 $，以便与用户定义的属性区分开来）。 beforeCreate：此时组件实例未创建，各个属性均没有生成。 第二步：Init、injections、 reactivity。属性均已注入绑定，而且被$watch变成reactivity。但是$el还是没有生成，也就是DOM没有生成； created：\\$data和message均已存在，\\$el还没有。 第三步：判断vue实例中是否有\\$el。如果有，则判断是否有template。如果没有则在手动挂载\\$el之后，再判断是否有template。 第四步： 1、在实例内部有template属性的时候，直接用内部的，然后调用render函数去渲染。 2、在实例内部没有找到template，就调用外部的html。实例内部的template属性比外部的优先级高。 3、要是前两者都不满足，那么就抛出错误。 beforeMount：只编译了模板，并没有挂载属性。即此时存在的还是虚拟DOM 第五步：创建vm.$el替换虚拟DOM（vm为初始化的实例对象）。 mounted：此时属性已挂载。\\$data、\\$el和message均处于已定义的状态。 beforeUpdate和updated：当数据改变时，这两个生命周期函数控制view层重新渲染。 渲染步骤：数据改变——导致虚拟DOM的改变——调用这两个生命钩子去改变视图 1、只有当数据与模板中的数据绑定才会这两个函数才会有效； 123456789101112131415var vm = new Vue({ el: '#app', template: '&lt;div id=\"app\"&gt;&lt;/div&gt;', // 这里需要是&lt;div id=\"app\"&gt;{{a}}&lt;/div&gt;才有效果 beforeUpdate: function() { console.log('调用了beforeUpdate') }, updated: function() { console.log('调用了uodated') }, data: { a: 1 }})vm.a = 2 // 虽然数据发生了改变，但是并未与模板绑定，所以控制台不会打印任何一条语句 2、数据改变后，在beforeUpdate和updated中分别console.log(this.$el)发现输出结果相同 beforeUpdate：数据更新时调用，发生在虚拟 DOM 重新渲染和打补丁之前。updated：由于数据更改导致的虚拟 DOM 重新渲染和打补丁，在这之后会调用该钩子。 this.$el是一个对象，或者说是一个指针。所以更新之后显示的都是一样的。可以通过 1console.log(\"真实的DOM结构: \"+ document.getElementById('app').innerHTML) 观察真实的DOM结构比对。实际中可以发现beforeUpdate中还是原来的数据，updated变成了之后的数据。 beforeDestory和destoryed：1、使用app.$destroy()进行销毁； 销毁后DOM元素仍然存在，但是再次改变data的值，beforeUpdate和updated均不起作用。即Vue 实例指示的所有东西都会解绑定，所有的事件监听器会被移除，所有的子实例也会被销毁 生命周期函数的使用场景： beforeCreate : 举个栗子：可以在这加个loading事件 created ：在这结束loading，还做一些初始化，实现函数自执行 mounted ： 在这发起后端请求，拿回数据，配合路由钩子做一些事情 beforeDestroy： 你确认删除XX吗？ destroyed ：当前组件已被删除，清空相关内容 示例代码：（监测\\$el、\\$data、\\$message变化）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt; &lt;meta charset=\"UTF-8\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"&gt; &lt;title&gt;vue生命周期学习&lt;/title&gt; &lt;script src=\"https://cdn.jsdelivr.net/npm/vue@2.5.17/dist/vue.js\"&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;div id=\"app\"&gt; &lt;h1&gt;{{message}}&lt;/h1&gt; &lt;/div&gt;&lt;/body&gt;&lt;script&gt; var vm = new Vue({ el: '#app', data: { message: 'Vue的生命周期' }, // 模板将会 替换 挂载的元素 beforeCreate: function() { console.group('------beforeCreate创建前状态------'); console.log(\"%c%s\", \"color:red\" , \"el : \" + this.$el); //undefined console.log(\"%c%s\", \"color:red\",\"data : \" + this.$data); //undefined console.log(\"%c%s\", \"color:red\",\"message: \" + this.message) }, created: function() { console.group('------created创建完毕状态------'); console.log(\"%c%s\", \"color:red\",\"el : \" + this.$el); //undefined console.log(\"%c%s\", \"color:red\",\"data : \" + this.$data); //已被初始化 console.log(\"%c%s\", \"color:red\",\"message: \" + this.message); //已被初始化 }, beforeMount: function() { console.group('------beforeMount挂载前状态------'); console.log(\"%c%s\", \"color:red\",\"el : \" + (this.$el)); //已被初始化 console.log(this.$el); console.log(\"%c%s\", \"color:red\",\"data : \" + this.$data); //已被初始化 console.log(\"%c%s\", \"color:red\",\"message: \" + this.message); //已被初始化 }, mounted: function() { console.group('------mounted 挂载结束状态------'); console.log(\"%c%s\", \"color:red\",\"el : \" + this.$el); //已被初始化 console.log(this.$el); console.log(\"%c%s\", \"color:red\",\"data : \" + this.$data); //已被初始化 console.log(\"%c%s\", \"color:red\",\"message: \" + this.message); //已被初始化 }, beforeUpdate: function () { console.group('beforeUpdate 更新前状态===============》'); console.log(\"真实的DOM结构: \"+ document.getElementById('app').innerHTML) console.log(\"%c%s\", \"color:red\",\"el : \" + this.$el); console.log(this.$el); console.log(\"%c%s\", \"color:red\",\"data : \" + this.$data); console.log(\"%c%s\", \"color:red\",\"message: \" + this.message); }, updated: function () { console.group('updated 更新完成状态===============》'); console.log(\"真实的DOM结构: \"+ document.getElementById('app').innerHTML) console.log(\"%c%s\", \"color:red\",\"el : \" + this.$el); console.log(this.$el); console.log(\"%c%s\", \"color:red\",\"data : \" + this.$data); console.log(\"%c%s\", \"color:red\",\"message: \" + this.message); }, beforeDestroy: function () { console.group('beforeDestroy 销毁前状态===============》'); console.log(\"%c%s\", \"color:red\",\"el : \" + this.$el); console.log(this.$el); console.log(\"%c%s\", \"color:red\",\"data : \" + this.$data); console.log(\"%c%s\", \"color:red\",\"message: \" + this.message); }, destroyed: function () { console.group('destroyed 销毁完成状态===============》'); console.log(\"%c%s\", \"color:red\",\"el : \" + this.$el); console.log(this.$el); console.log(\"%c%s\", \"color:red\",\"data : \" + this.$data); console.log(\"%c%s\", \"color:red\",\"message: \" + this.message) } })&lt;/script&gt;&lt;/html&gt; 参考文献 详解vue生命周期 - SegmentFault思否 API - Vue.js vue生命周期详解 - 掘金 vue2 为什么beforeUpdate时的$el 和$data与updated时的一样","link":"/2018/06/10/Vue生命周期解析/"},{"title":"静态资源加速","text":"网站的静态资源介绍在使用浏览器控制台观测各个网站的详细信息时，可以看到head标签中存在大量引用的js、css或其它类型的文件。如下所示：1234&lt;!--指向站点内部的文件--&gt;&lt;meta content=\"/images/branding/googleg/1x/googleg_standard_color_128dp.png\" itemprop=\"image\"&gt;&lt;!--绝对路径指向外部站点的js--&gt;&lt;script src=\"http://apps.bdimg.com/libs/bootstrap/3.3.0/js/bootstrap.min.js\"&gt;&lt;/script&gt; 如果网站规模较小、资源请求数少、对存储空间、并发量的要求不高，可以尝试将资源代码与业务代码存放在一起的方式。如果网站规模扩大，优化的方式也有许多，单从静态资源的角度看可以将资源服务器与业务服务器分离。这就引入了CDN的概念 CDN（Content Delivery Network）CDN即内容分发网络。通俗意义上说就是能够让你访问网站的速度变快。可以将资源类型的文件存储在CDN服务器中，CDN系统根据当前互联网运行的各项指标如节点的连接、负载、到用户的响应时间和距离，将请求导向离用户最近的服务节点上。这样既保证了访问的稳定性与速度也降低了业务服务器的资源流量。国内做CDN的厂商有阿里云、百度、七牛、又拍等。阿里云的售后保障是比较靠谱的，当然价格也相对较高，其他公司的产品没用过，不做评价。其中有一家叫白山云的公司，产品好不好不知道，但面试体验是极好的。（可惜没有谁给我广告费） 图片资源的处理静态站点中如果存在的大量图片，那么图片请求时间过长会导致站点的访问速度变慢。提高网站的图片加载速度当然也可以使用CDN，同时也可以对图片本身进行压缩合并。这里提供一种思路：本站的原始背景图片是大小为4M左右格式为png的图片。经压缩后大小为1M左右，格式仍为png，肉眼看不出变化，压缩率达到75%左右。压缩站点TinyPNG。但是实测中访问速度不是很明显。所以再次将图片压缩为webP格式，肉眼仍看不出变化，压缩度达到90%左右。压缩站点Squoosh。但是该图片格式由谷歌推出，Chrome与Opera能够实现支持，FireFox暂不可以（据说可能需要到明年上半年）。所以使用Firefox等浏览器访问得到的是#e7e7e7为背景色的网站。 参考文献 CDN是什么？使用CDN有什么优势？ - 知乎 WebP 相对于 PNG、JPG 有什么优势？- 知乎 网站加载图片速度慢如何优化 - SegmentFault 思否 webP - 维基百科","link":"/2018/05/17/静态资源加速/"},{"title":"抓取网易云音乐榜单","text":"摘要本文分别利用Selenium自动化测试工具与requests库抓取网易云音乐的榜单，包括歌曲名、歌曲时长以及演唱者等信息。Selenium仅仅抓取了首个榜单，requests抓取了所有榜单。抓取之后将所有数据存入MySQL数据库中。 Selenium抓取单个表单Selenium是一种自动化测试工具，它可以模拟浏览器的行为，所以也可以用于爬虫。下载对应的驱动之后，可以调用各种主流浏览器。一般在爬虫的过程因为效率问题并不打开浏览器，而使用无界面的模式。 执行过程1、初始化变量初始化的过程中包括请求的url，请求的数目以及数据库的一些配置。selenium使用Chrome驱动。1234567891011121314def __init__(self): self.url = \"https://music.163.com\" self.count = 0 self.host = \"127.0.0.1\" self.user = \"root\" self.password = \"******\" self.db = \"spider\" self.chrome_option = Options() self.chrome_option.add_argument('--headless') self.DRIVER = webdriver.Chrome(chrome_options=self.chrome_option) # The format string is not really a normal Python format string. # You must always use %s for all fields. self.importSQL = \"\"\"INSERT INTO NetEaseCloudMusic(ID, musicName, time, singer) VALUES (%s, %s, %s, %s)\"\"\" 2、连接MySQL数据库，并从首页跳转到榜单页根据初始化的数据库配置信息，连接上数据库删除旧表，创建新表12345678910111213def connect_mysql(self): self.db = pymysql.connect(host=self.host, user=self.user, passwd=self.password, db=self.db) print(\"成功连接Mysql数据库\") self.cursor = self.db.cursor() self.cursor.execute(\"DROP TABLE IF EXISTS NetEaseCloudMusic\") # 如果使用单引号，需要 sql = '''CREATE TABLE NetEaseCloudMusic( ID INT NOT NULL, musicName LONGTEXT, time LONGTEXT, singer LONGTEXT )''' self.cursor.execute(sql) 榜单信息的url为https://music.163.com/#/discover/toplist，所以需要从首页跳转。跳转之后观察到榜单信息并不是直接加载在网页中，而是在页面的iframe标签中，所以需要跳转到子iframe拿到数据123456789101112131415def jump_targetpage(self): # 其中 driver.get 方法会打开请求的URL， # WebDriver 会等待页面完全加载完成之后才会返回， # 即程序会等待页面的所有内容加载完成，JS渲染完毕之后才继续往下执行。 self.DRIVER.get(self.url) self.DRIVER.find_element_by_xpath('//ul[@class=\"nav\"]/li[2]').click() try: self.firstChild_iframe = self.DRIVER.find_element_by_id(\"g_iframe\") except Exception as e: print(\"Get iframe Failed: \", e) sleep(2) self.DRIVER.switch_to.frame(self.firstChild_iframe) table = self.DRIVER.find_element_by_tag_name('table') self.songList = table.find_elements_by_xpath('tbody/tr') return self.songList 3、提取数据并保存到数据库中使用xpath来提取歌曲名称、播放时长、演唱者等信息。123456def start_search(self, song): songChildList = [] songChildList.append(song.find_element_by_xpath('td[2]//b').get_attribute('title')) songChildList.append(song.find_element_by_xpath('td[3]//span[@class=\"u-dur \"]').text) songChildList.append(song.find_element_by_xpath('td[last()]//span').get_attribute('title')) return songChildList 保存到数据库中，并记录每次导入的条数123456789def import_data(self, data): try: self.cursor.execute(self.importSQL, (self.count+1, data[0], data[1], data[2])) self.db.commit() self.count += 1 print(\"成功导入第{count}条数据\".format(count=self.count)) except Exception as e: print('导入数据失败： ', e) self.db.rollback() 小结这里并不介绍Selenium的具体使用方式，侧重于抓取的过程描述以及如何实现这一过程的简单代码示例。完整代码：Selenium抓取单个表单 requests抓取所有表单分析网易云音乐榜单页面的网络请求，如图所示： 从默认页面https://music.163.com/#/discover/toplist请求的响应信息可以看到各类榜单的标题以及href属性信息。事实上，通过观察浏览器地址栏的url可以看出href的属性值位于#之后，所以这是一个fragment，访问网站之后可以直接到达页面的指定位置。我们需要提取出每个榜单的href属性信息，再逐个请求这些url，通过返回的响应分析出每个url中包含的歌曲信息，再逐个保存到数据库中，并统计总的数据量。 请求过程初始化变量以及连接数据库的部分在使用selenium请求中已经介绍过，大同小异。此处不再过多介绍，在文章结尾处会给出源代码的链接。关于发起请求的函数部分，由于请求的url包括初始的url以及每个榜单的url，所以在请求函数中考虑使用可变数量的参数，如果没有传入其他参数则使用初始化的url，否则使用传入的url。过程如下： 123456789101112131415def start_request(self, *arg): # 向给定的url请求数据，如果arg为空，请求原始的url，否则请求arg url = arg if arg else self.url if isinstance(url, tuple): # arg是元组类型 request_url = url[0] else: request_url = url try: response = requests.get(request_url, headers=self.header) if response.status_code == 200: print(\"请求成功\") return response except Exception as e: print(\"请求失败，失败原因：\", e) 处理原始请求的数据对原始的url发送请求时，使用正则表达式提取所有榜单的href属性，再与原始的url进行拼接（因为拼接的url即使不带有#也会自动跳转，所以代码中并未添加#），得到并请求单个榜单的url。单个榜单的url中并不能够找到规则的歌曲数据。通过观察发现所有歌曲数据都存在其中一个textarea的标签中，标签中的数据为json格式，所以需要利用这些json数据提取歌曲信息。这里通过正则表达式将其提取出来。 123456789101112131415161718def process_data(self, source_response): # 处理原始url的请求，提取并处理每个榜单的url pattern = re.compile('&lt;p class=\"name\"&gt;&lt;a href=\"(.*?)\" class=\"s-fc0\"&gt;') lists_link = pattern.findall(source_response.text) for list_link in lists_link: url = parse.urljoin(self.url, list_link) list_response = self.start_request(url) json_datas = self.process_jsondata(list_response) self.process_musicdata(json_datas)def process_jsondata(self, response): # 提取每个榜单的json数据 pattern = re.compile('.*?id=\"song-list-pre-data\".*?&gt;(.*?)&lt;/textarea&gt;') match = pattern.search(response.text) json_datas = json.loads(match.group(1)) with open('data.json', 'wb') as f: f.write(match.group(1).encode('utf8')) return json_datas 处理包含歌曲信息的json数据歌曲名与演唱者都可以直接从json中直接方便的读取到，但是歌曲的时长并不能从数据中直接看出来。所以回到原始榜单的响应中寻找信息，发现了可能是控制歌曲时长显示的部分，如图所示： 以下语句为控制歌曲时长显示的代码：1${dur2time(x.duration/1000)} 实际上我并没有在所有网络请求中找到有关dur2time的信息（也可能是我还不知道怎样分析这一部分）。但在json数据中存在duration的key，所以我认为duration的数值可能会通过某种转换，显示为歌曲时长。将duration的值除以1000与页面中显示的歌曲时长对应起来，试着找到其中的规律。 179.2 &nbsp;&nbsp; 02：59 216 &nbsp;&nbsp; 03：36 从以上的对应关系中基本上可以判断duration除以1000即是歌曲时长的秒数。到此可以便可以对歌曲的数据进行提取。过程如下所示： 12345678910111213141516171819def process_musicdata(self, json_datas): # 处理每个榜单的json数据 for json_data in json_datas: music_name = json_data.get(\"name\") play_time = self.process_playtime(json_data.get(\"duration\")/1000) singers = '' for artist in json_data.get(\"artists\"): singers += '/' + artist.get(\"name\") singers = singers.replace('/', '', 1) data = [music_name, play_time, singers] self.save_data(data)def process_playtime(self, duration): # 处理播放时间数据 minutes = math.floor(duration/60) seconds = math.floor(duration%60) minutes = minutes if minutes &gt;= 10 else '0'+str(minutes) seconds = seconds if seconds &gt;= 10 else '0'+str(seconds) return '%s:%s' % (minutes, seconds) 保存到数据库中每次处理一个榜单的响应便保存到数据库中。本次提取的所有歌曲榜单，共有1614条音乐数据12345678910def save_data(self, data): # 保存数据到数据库 try: self.cursor.execute(self.importSQL, (self.count, data[0],data[1],data[2])) self.db.commit() print(\"成功导入第{count}条数据\".format(count=self.count)) self.count += 1 except Exception as e: print(\"导入数据失败，失败原因：\", e) self.db.rollback() 总结本文分别使用了Selenium与requests两种方式抓取音乐榜单数据。实际测试中Selenium的抓取速度较慢，但是过程相对简单。使用requests请求过程稍显复杂，但是速度较快。完整代码：抓取网易云音乐榜单 参考文献1、Selenium with Python Selenium Python Bindings 2 documentation2、Getting Started with Headless Chrome | Web | Google Developers3、2. 快速入门 Selenium-Python中文文档 2 documentation4、python3 urllib.parse.urljoin（）用法 - 余安 - CSDN博客5、20.7. urllib.parse — Parse URLs into components &mdash; Python v3.2.6 documentation","link":"/2018/12/20/抓取网易云音乐榜单/"}],"tags":[{"name":"Echarts","slug":"Echarts","link":"/tags/Echarts/"},{"name":"Javascript","slug":"Javascript","link":"/tags/Javascript/"},{"name":"念念不忘，必有回响","slug":"念念不忘，必有回响","link":"/tags/念念不忘，必有回响/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"验证码","slug":"验证码","link":"/tags/验证码/"},{"name":"cookies","slug":"cookies","link":"/tags/cookies/"},{"name":"模拟登录","slug":"模拟登录","link":"/tags/模拟登录/"},{"name":"数据结构","slug":"数据结构","link":"/tags/数据结构/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"二叉树","slug":"二叉树","link":"/tags/二叉树/"},{"name":"ajax","slug":"ajax","link":"/tags/ajax/"},{"name":"MongoDB","slug":"MongoDB","link":"/tags/MongoDB/"},{"name":"正则表达式","slug":"正则表达式","link":"/tags/正则表达式/"},{"name":"词云","slug":"词云","link":"/tags/词云/"},{"name":"代理","slug":"代理","link":"/tags/代理/"},{"name":"Vue","slug":"Vue","link":"/tags/Vue/"},{"name":"图片压缩","slug":"图片压缩","link":"/tags/图片压缩/"},{"name":"CDN","slug":"CDN","link":"/tags/CDN/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"Selenium","slug":"Selenium","link":"/tags/Selenium/"}],"categories":[{"name":"建站实录","slug":"建站实录","link":"/categories/建站实录/"},{"name":"念念不忘，必有回响","slug":"念念不忘，必有回响","link":"/categories/念念不忘，必有回响/"},{"name":"数据抓取","slug":"数据抓取","link":"/categories/数据抓取/"},{"name":"数据结构与算法","slug":"数据结构与算法","link":"/categories/数据结构与算法/"},{"name":"足迹图","slug":"建站实录/足迹图","link":"/categories/建站实录/足迹图/"},{"name":"豆瓣","slug":"数据抓取/豆瓣","link":"/categories/数据抓取/豆瓣/"},{"name":"二分查找","slug":"数据结构与算法/二分查找","link":"/categories/数据结构与算法/二分查找/"},{"name":"二叉树","slug":"数据结构与算法/二叉树","link":"/categories/数据结构与算法/二叉树/"},{"name":"知乎","slug":"数据抓取/知乎","link":"/categories/数据抓取/知乎/"},{"name":"正则表达式","slug":"数据抓取/豆瓣/正则表达式","link":"/categories/数据抓取/豆瓣/正则表达式/"},{"name":"Vue","slug":"Vue","link":"/categories/Vue/"},{"name":"静态资源加速","slug":"建站实录/静态资源加速","link":"/categories/建站实录/静态资源加速/"},{"name":"基础原理类","slug":"Vue/基础原理类","link":"/categories/Vue/基础原理类/"},{"name":"网易云音乐","slug":"数据抓取/网易云音乐","link":"/categories/数据抓取/网易云音乐/"}]}