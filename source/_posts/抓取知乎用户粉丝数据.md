---
title: 抓取知乎用户粉丝数据
layout: post
date: 2018-12-13 08:44:41
categories: 
- 数据抓取
- 知乎
tags:
- Python
- Echarts
- 正则表达式
- 词云
cover: https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/CoverPicture/bg_9.png
---

## 摘要
本文抓取并分析了知乎用户的粉丝数据，包括粉丝名、粉丝标题、粉丝男女比例、粉丝回答问题比例、粉丝被关注数等数据。之后利用`pyecharts`库对这些数据进行可视化显示。
<br/>
## 抓取数据
### 1、请求的url与代理
与之前一篇文章[抓取知乎用户动态数据](http://yaodongsheng.com/2018/11/20/抓取知乎用户动态数据/)类似，这里请求的url也是返回json数据，所以同样在控制台network选项下找到请求的url，通过观察多个请求的url将其分为通用部分与扩展部分，如下形式：
```Python
self.base_url = "https://www.zhihu.com/api/v4/members/{user}/followers?{include}"
self.include = 'include=data%5B*%5D.answer_count%2Carticles_count%2C\
  gender%2Cfollower_count%2Cis_followed%2Cis_following%2C\
  badge%5B%3F(type%3Dbest_answerer)%5D.topics&offset={offset}&limit=20'
```
其中`user`为用户的个性化域名，`include`为扩展部分的url，`offset`为偏移量，即从第`offset`条数据开始。
代理使用[fake-useragent](https://pypi.org/project/fake-useragent/)库，每次请求都随机生成，保证每次请求的`header`都不一样。
<br/>
### 2、设置循环抓取
首先根据url请求`json`数据，之后处理请求到的数据，得到一部分粉丝的信息，如果粉丝信息没有抓取完，则对抓取的`url`进行更新，直到数据抓取完成，结束的标志会在请求的`json`数据中获取到。最后对提取到的所有数据进行保存。代码如下：
```Python
def start(self, username, savepath, savename):
  index = 0
  flag = True
  while flag:
    offset=index*20
    include = self.include.format(offset=offset)
    url = self.base_url.format(user=username, include=include)
    # 请求json数据
    print("正在进行第{}次请求".format(index+1))
    jsonData = self.requests_source(url)
    # 提取需要的数据
    self.process_data(jsonData)
    # 如果仍然存在数据，处理下一次请求
    if jsonData.get('paging').get('is_end') is False:
      index = index + 1
      self.header['user-agent'] = self.agent.random
    else:
      flag = False
      print("完成请求")
  # 保存数据
  self.save_data(self.data, savepath, savename)
```
<br/>
### 3、请求、处理数据
请求处理的过程并不复杂，根据返回的状态码判断是否成功拿到`json`，之后会根据`json`的具体形式解析数据。
```Python
def requests_source(self, url):
  try:
    response = requests.get(url, headers=self.header, timeout=(5,30))
    if response.status_code == 200:
      return response.json()
  except requests.ConnectionError as e:
    print("获取请求失败，失败原因：", e)

def process_data(self, jsonData):
  followers = jsonData.get('data')
  for follow in followers:
    name = self.read_data_from_jsonData(follow, 'name')
    headline = self.read_data_from_jsonData(follow, 'headline')
    gender = self.read_data_from_jsonData(follow, 'gender')
    follower_count = self.read_data_from_jsonData(follow, 'follower_count')
    answer_count = self.read_data_from_jsonData(follow, 'answer_count')
    self.data[name] = [headline, gender, follower_count, answer_count]
    print(name, self.data[name])
  print('\n')

def read_data_from_jsonData(self, json, key):
  value = json.get(key)
  return value if value is not '' else 'Empty'
```
<br/>
### 4、保存数据
这里将所有信息保存到`pkl`文件当中
```Python
def save_data(self, follower_data, savepath, savename):
  if not os.path.exists(savepath):
    os.mkdir(savepath)
  with open(os.path.join(savepath, savename), 'wb') as f:
    pickle.dump(follower_data, f)
  f.close()
  print("完成存储")
```
完成所有信息的抓取、处理及保存之后下一步就是解析可视化这些数据。
<br/>
## 可视化数据
可视化数据利用了[pyecharts](http://pyecharts.org/)库与[jieba](https://pypi.org/project/jieba/)库，前者用于可视化各类图表，后者是Python的中文分词组件。
<br/>
### 1、打开`pkl`文件
保存`pkl`文件的名称可以自行指定，每次抓取如果文件名称改变那么分析数据的时候需要修改代码，所以使用了正则匹配，这样无需关心pkl文件的名称就可以直接运行。代码如下：
```Python
for root, dirs, files in os.walk(os.path.join(os.path.dirname(__file__),'result')):
  # print(root)  # 当前目录路径
  # print(dirs)  # 当前路径下所有子目录
  # print(files)  # 当前路径下所有非目录子文件
  for file in files:
    try:
      file_name = re.search('(.*?).pkl', file).group()
      break
    except Exception as e:
      continue
with open('./result/'+file_name, 'rb') as f:
  followers_data = pickle.load(f)
```
<br/>
### 2、提取数据
为了便于分析、可视化数据，所以会将所有数据提取成`[(元组1), (元组2),...,(元组3)]`的形式，例如性别数据的形式为
```Python
[('男性': 32),('女性': 42),('保密': 52)]
```

生成`headline`与`用户名`的词云图时使用`jieba`库分词，其中在处理`headline`时需要删除掉一些符号字符之后再统计词频
```Python
def statistic_word_frequency(self, texts, stopwords):
  # 统计词频
  statistic_dict = {}
  for text in texts:
    temp = jieba.cut(text, cut_all=False)
    for t in temp:
      if t in stopwords or t == 'Empty':
        continue;
      if t in statistic_dict.keys():
        statistic_dict[t] += 1
      else:
        statistic_dict[t] = 1
  return list(statistic_dict.items())
```
stopwords用来存储需要删除掉的一些字符。
<br/>

### 3、示例图
示例图是随机寻找的一位知乎用户进行分析的结果
![标题词云图](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取知乎用户粉丝数据/标题词云图.png)
![用户名词云图](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取知乎用户粉丝数据/用户名词云图.png)
![粉丝被关注数量直方图](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取知乎用户粉丝数据/粉丝被关注数量直方图.png)
![粉丝回答数量直方图](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取知乎用户粉丝数据/粉丝回答数量直方图.png)
![粉丝男女比例饼图](https://cdn.jsdelivr.net/gh/Cloving/Atlas-Github/blog/notePicture/抓取知乎用户粉丝数据/粉丝男女比例饼图.png)

<br/>
## 总结
本文的思路参考了知乎用户[Charles](https://www.zhihu.com/people/charles_pikachu/activities)的爬虫项目。最终完成的代码：[知乎用户粉丝](https://github.com/Cloving/zhihu-Spider/tree/master/%E7%9F%A5%E4%B9%8E%E7%94%A8%E6%88%B7%E7%B2%89%E4%B8%9D)


## 参考文献
1、[Charles的皮卡丘](https://mp.weixin.qq.com/s/hWUvMFIAMBrgR2Is9P5QXw)
2、[pyecharts - A Python Echarts Plotting Library](http://pyecharts.org/#/zh-cn/charts_base)
3、[fxsjy/jieba: 结巴中文分词](https://github.com/fxsjy/jieba)
